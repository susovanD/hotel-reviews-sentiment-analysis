2024-09-27 16:18:17,597:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-27 16:18:17,597:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-27 16:18:17,597:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-27 16:18:17,597:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-27 16:19:41,714:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-27 16:19:41,714:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-27 16:19:41,714:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-27 16:19:41,714:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-27 16:35:09,850:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-27 16:35:09,851:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-27 16:35:09,851:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-27 16:35:09,851:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-27 16:35:57,472:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-27 16:35:57,472:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-27 16:35:57,472:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-27 16:35:57,472:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-27 17:27:48,114:INFO:PyCaret ClassificationExperiment
2024-09-27 17:27:48,117:INFO:Logging name: clf-default-name
2024-09-27 17:27:48,117:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-27 17:27:48,117:INFO:version 3.3.2
2024-09-27 17:27:48,117:INFO:Initializing setup()
2024-09-27 17:27:48,117:INFO:self.USI: d401
2024-09-27 17:27:48,117:INFO:self._variable_keys: {'fix_imbalance', 'gpu_param', 'pipeline', 'X', 'is_multiclass', 'log_plots_param', 'seed', 'X_train', 'gpu_n_jobs_param', 'data', 'y_test', 'exp_id', 'y', 'html_param', 'exp_name_log', '_ml_usecase', 'X_test', 'n_jobs_param', '_available_plots', 'USI', 'fold_generator', 'fold_groups_param', 'idx', 'target_param', 'logging_param', 'fold_shuffle_param', 'memory', 'y_train'}
2024-09-27 17:27:48,118:INFO:Checking environment
2024-09-27 17:27:48,120:INFO:python_version: 3.11.9
2024-09-27 17:27:48,120:INFO:python_build: ('tags/v3.11.9:de54cf5', 'Apr  2 2024 10:12:12')
2024-09-27 17:27:48,120:INFO:machine: AMD64
2024-09-27 17:27:48,120:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-27 17:27:48,196:INFO:Memory: svmem(total=8434716672, available=579178496, percent=93.1, used=7855538176, free=579178496)
2024-09-27 17:27:48,196:INFO:Physical Core: 4
2024-09-27 17:27:48,197:INFO:Logical Core: 8
2024-09-27 17:27:48,197:INFO:Checking libraries
2024-09-27 17:27:48,198:INFO:System:
2024-09-27 17:27:48,200:INFO:    python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]
2024-09-27 17:27:48,200:INFO:executable: d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Scripts\python.exe
2024-09-27 17:27:48,200:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-27 17:27:48,200:INFO:PyCaret required dependencies:
2024-09-27 17:27:48,688:INFO:                 pip: 24.0
2024-09-27 17:27:48,688:INFO:          setuptools: 65.5.0
2024-09-27 17:27:48,688:INFO:             pycaret: 3.3.2
2024-09-27 17:27:48,688:INFO:             IPython: 8.27.0
2024-09-27 17:27:48,688:INFO:          ipywidgets: 8.1.5
2024-09-27 17:27:48,688:INFO:                tqdm: 4.66.5
2024-09-27 17:27:48,688:INFO:               numpy: 1.26.4
2024-09-27 17:27:48,688:INFO:              pandas: 2.1.4
2024-09-27 17:27:48,688:INFO:              jinja2: 3.1.4
2024-09-27 17:27:48,688:INFO:               scipy: 1.11.4
2024-09-27 17:27:48,688:INFO:              joblib: 1.3.2
2024-09-27 17:27:48,688:INFO:             sklearn: 1.4.2
2024-09-27 17:27:48,688:INFO:                pyod: 2.0.2
2024-09-27 17:27:48,688:INFO:            imblearn: 0.12.3
2024-09-27 17:27:48,688:INFO:   category_encoders: 2.6.3
2024-09-27 17:27:48,689:INFO:            lightgbm: 4.5.0
2024-09-27 17:27:48,689:INFO:               numba: 0.60.0
2024-09-27 17:27:48,689:INFO:            requests: 2.32.3
2024-09-27 17:27:48,689:INFO:          matplotlib: 3.7.5
2024-09-27 17:27:48,689:INFO:          scikitplot: 0.3.7
2024-09-27 17:27:48,689:INFO:         yellowbrick: 1.5
2024-09-27 17:27:48,689:INFO:              plotly: 5.24.1
2024-09-27 17:27:48,689:INFO:    plotly-resampler: Not installed
2024-09-27 17:27:48,689:INFO:             kaleido: 0.2.1
2024-09-27 17:27:48,689:INFO:           schemdraw: 0.15
2024-09-27 17:27:48,689:INFO:         statsmodels: 0.14.3
2024-09-27 17:27:48,689:INFO:              sktime: 0.26.0
2024-09-27 17:27:48,689:INFO:               tbats: 1.1.3
2024-09-27 17:27:48,689:INFO:            pmdarima: 2.0.4
2024-09-27 17:27:48,689:INFO:              psutil: 6.0.0
2024-09-27 17:27:48,689:INFO:          markupsafe: 2.1.5
2024-09-27 17:27:48,689:INFO:             pickle5: Not installed
2024-09-27 17:27:48,689:INFO:         cloudpickle: 3.0.0
2024-09-27 17:27:48,689:INFO:         deprecation: 2.1.0
2024-09-27 17:27:48,689:INFO:              xxhash: 3.5.0
2024-09-27 17:27:48,690:INFO:           wurlitzer: Not installed
2024-09-27 17:27:48,690:INFO:PyCaret optional dependencies:
2024-09-27 17:27:48,708:INFO:                shap: Not installed
2024-09-27 17:27:48,708:INFO:           interpret: Not installed
2024-09-27 17:27:48,708:INFO:                umap: Not installed
2024-09-27 17:27:48,708:INFO:     ydata_profiling: Not installed
2024-09-27 17:27:48,708:INFO:  explainerdashboard: Not installed
2024-09-27 17:27:48,708:INFO:             autoviz: Not installed
2024-09-27 17:27:48,708:INFO:           fairlearn: Not installed
2024-09-27 17:27:48,708:INFO:          deepchecks: Not installed
2024-09-27 17:27:48,709:INFO:             xgboost: Not installed
2024-09-27 17:27:48,709:INFO:            catboost: Not installed
2024-09-27 17:27:48,709:INFO:              kmodes: Not installed
2024-09-27 17:27:48,709:INFO:             mlxtend: Not installed
2024-09-27 17:27:48,709:INFO:       statsforecast: Not installed
2024-09-27 17:27:48,709:INFO:        tune_sklearn: Not installed
2024-09-27 17:27:48,709:INFO:                 ray: Not installed
2024-09-27 17:27:48,709:INFO:            hyperopt: Not installed
2024-09-27 17:27:48,709:INFO:              optuna: Not installed
2024-09-27 17:27:48,709:INFO:               skopt: Not installed
2024-09-27 17:27:48,709:INFO:              mlflow: Not installed
2024-09-27 17:27:48,709:INFO:              gradio: Not installed
2024-09-27 17:27:48,709:INFO:             fastapi: Not installed
2024-09-27 17:27:48,709:INFO:             uvicorn: Not installed
2024-09-27 17:27:48,709:INFO:              m2cgen: Not installed
2024-09-27 17:27:48,709:INFO:           evidently: Not installed
2024-09-27 17:27:48,709:INFO:               fugue: Not installed
2024-09-27 17:27:48,709:INFO:           streamlit: Not installed
2024-09-27 17:27:48,709:INFO:             prophet: Not installed
2024-09-27 17:27:48,709:INFO:None
2024-09-27 17:27:48,711:INFO:Set up data.
2024-09-27 17:27:48,963:INFO:Set up folding strategy.
2024-09-27 17:27:48,967:INFO:Set up train/test split.
2024-09-27 17:27:49,048:INFO:Set up index.
2024-09-27 17:27:49,049:INFO:Assigning column types.
2024-09-27 17:27:49,057:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-27 17:27:49,126:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-27 17:27:49,151:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-27 17:27:49,257:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-27 17:27:49,258:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-27 17:27:49,321:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-27 17:27:49,321:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-27 17:27:49,350:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-27 17:27:49,351:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-27 17:27:49,351:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-27 17:27:49,414:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-27 17:27:49,461:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-27 17:27:49,464:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-27 17:27:49,570:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-27 17:27:49,625:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-27 17:27:49,625:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-27 17:27:49,625:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-27 17:27:49,783:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-27 17:27:49,783:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-27 17:27:49,874:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-27 17:27:49,874:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-27 17:27:49,892:INFO:Preparing preprocessing pipeline...
2024-09-27 17:27:49,894:INFO:Set up label encoding.
2024-09-27 17:27:49,895:INFO:Set up simple imputation.
2024-09-27 17:27:49,899:INFO:Set up encoding of categorical features.
2024-09-27 17:27:50,023:INFO:Finished creating preprocessing pipeline.
2024-09-27 17:27:50,033:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ASUS\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              m...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('rest_encoding',
                 TransformerWrapper(exclude=None, include=['clened_review'],
                                    transformer=TargetEncoder(cols=['clened_review'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False)
2024-09-27 17:27:50,033:INFO:Creating final display dataframe.
2024-09-27 17:27:50,174:INFO:Setup _display_container:                     Description                                 Value
0                    Session id                                   123
1                        Target                       sentiment_final
2                   Target type                            Multiclass
3                Target mapping  Negative: 0, Neutral: 1, Positive: 2
4           Original data shape                              (195, 2)
5        Transformed data shape                              (195, 2)
6   Transformed train set shape                              (136, 2)
7    Transformed test set shape                               (59, 2)
8          Categorical features                                     1
9                    Preprocess                                  True
10              Imputation type                                simple
11           Numeric imputation                                  mean
12       Categorical imputation                                  mode
13     Maximum one-hot encoding                                    25
14              Encoding method                                  None
15               Fold Generator                       StratifiedKFold
16                  Fold Number                                    10
17                     CPU Jobs                                    -1
18                      Use GPU                                 False
19               Log Experiment                                 False
20              Experiment Name                      clf-default-name
21                          USI                                  d401
2024-09-27 17:27:50,322:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-27 17:27:50,322:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-27 17:27:50,407:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-27 17:27:50,407:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-27 17:27:50,415:INFO:setup() successfully completed in 2.42s...............
2024-09-27 17:29:04,080:INFO:Initializing compare_models()
2024-09-27 17:29:04,080:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EC22FAD0>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EC22FAD0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-27 17:29:04,080:INFO:Checking exceptions
2024-09-27 17:29:04,089:INFO:Preparing display monitor
2024-09-27 17:29:04,196:INFO:Initializing Logistic Regression
2024-09-27 17:29:04,196:INFO:Total runtime is 0.0 minutes
2024-09-27 17:29:04,202:INFO:SubProcess create_model() called ==================================
2024-09-27 17:29:04,207:INFO:Initializing create_model()
2024-09-27 17:29:04,208:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EC22FAD0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C1ED9282D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 17:29:04,209:INFO:Checking exceptions
2024-09-27 17:29:04,209:INFO:Importing libraries
2024-09-27 17:29:04,209:INFO:Copying training dataset
2024-09-27 17:29:04,214:INFO:Defining folds
2024-09-27 17:29:04,214:INFO:Declaring metric variables
2024-09-27 17:29:04,220:INFO:Importing untrained model
2024-09-27 17:29:04,233:INFO:Logistic Regression Imported successfully
2024-09-27 17:29:04,248:INFO:Starting cross validation
2024-09-27 17:29:04,251:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-27 17:29:04,326:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.
  warnings.warn(

2024-09-27 17:29:13,283:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:13,289:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:13,310:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:13,312:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:13,314:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:13,341:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:13,345:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:13,342:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:13,347:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:13,351:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:13,357:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:13,360:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:13,363:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:13,372:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:13,375:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:13,380:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:13,383:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:13,388:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:13,390:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:13,391:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:13,391:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:13,394:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:13,394:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:13,398:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:13,475:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:13,489:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:13,491:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:13,501:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:13,511:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:13,511:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:13,520:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:13,523:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:13,540:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:13,559:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:13,566:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:13,593:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:13,596:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:13,625:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:13,718:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:13,723:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:13,723:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:13,726:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:13,726:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:13,729:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:13,730:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:13,732:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:13,733:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:13,736:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:13,738:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:13,739:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:13,741:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:13,744:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:13,746:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:13,750:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:13,754:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:13,757:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:13,760:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:13,771:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:13,774:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:13,807:INFO:Calculating mean and std
2024-09-27 17:29:13,815:INFO:Creating metrics dataframe
2024-09-27 17:29:13,823:INFO:Uploading results into container
2024-09-27 17:29:13,824:INFO:Uploading model into container now
2024-09-27 17:29:13,827:INFO:_master_model_container: 1
2024-09-27 17:29:13,827:INFO:_display_container: 2
2024-09-27 17:29:13,828:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-27 17:29:13,832:INFO:create_model() successfully completed......................................
2024-09-27 17:29:14,941:INFO:SubProcess create_model() end ==================================
2024-09-27 17:29:14,942:INFO:Creating metrics dataframe
2024-09-27 17:29:14,949:INFO:Initializing K Neighbors Classifier
2024-09-27 17:29:14,949:INFO:Total runtime is 0.1792197028795878 minutes
2024-09-27 17:29:14,952:INFO:SubProcess create_model() called ==================================
2024-09-27 17:29:14,952:INFO:Initializing create_model()
2024-09-27 17:29:14,952:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EC22FAD0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C1ED9282D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 17:29:14,952:INFO:Checking exceptions
2024-09-27 17:29:14,952:INFO:Importing libraries
2024-09-27 17:29:14,952:INFO:Copying training dataset
2024-09-27 17:29:14,957:INFO:Defining folds
2024-09-27 17:29:14,957:INFO:Declaring metric variables
2024-09-27 17:29:14,959:INFO:Importing untrained model
2024-09-27 17:29:14,962:INFO:K Neighbors Classifier Imported successfully
2024-09-27 17:29:14,969:INFO:Starting cross validation
2024-09-27 17:29:14,970:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-27 17:29:14,973:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.
  warnings.warn(

2024-09-27 17:29:15,111:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 17:29:15,113:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 17:29:15,113:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 17:29:15,115:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,115:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,116:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,116:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,117:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,117:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,117:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,118:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,119:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,119:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,119:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,120:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,120:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,121:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,122:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,122:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,122:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,122:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,123:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,123:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,124:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,124:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,125:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,125:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,125:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,125:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,126:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,127:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,127:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,127:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,127:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,127:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,131:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,132:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,134:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,135:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,137:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,190:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 17:29:15,190:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 17:29:15,193:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,194:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,196:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,197:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,198:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,198:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,200:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,200:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,205:INFO:Calculating mean and std
2024-09-27 17:29:15,206:INFO:Creating metrics dataframe
2024-09-27 17:29:15,208:INFO:Uploading results into container
2024-09-27 17:29:15,209:INFO:Uploading model into container now
2024-09-27 17:29:15,209:INFO:_master_model_container: 2
2024-09-27 17:29:15,209:INFO:_display_container: 2
2024-09-27 17:29:15,209:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-27 17:29:15,209:INFO:create_model() successfully completed......................................
2024-09-27 17:29:15,284:INFO:SubProcess create_model() end ==================================
2024-09-27 17:29:15,285:INFO:Creating metrics dataframe
2024-09-27 17:29:15,300:INFO:Initializing Naive Bayes
2024-09-27 17:29:15,300:INFO:Total runtime is 0.18507131338119506 minutes
2024-09-27 17:29:15,304:INFO:SubProcess create_model() called ==================================
2024-09-27 17:29:15,305:INFO:Initializing create_model()
2024-09-27 17:29:15,305:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EC22FAD0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C1ED9282D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 17:29:15,305:INFO:Checking exceptions
2024-09-27 17:29:15,305:INFO:Importing libraries
2024-09-27 17:29:15,305:INFO:Copying training dataset
2024-09-27 17:29:15,308:INFO:Defining folds
2024-09-27 17:29:15,308:INFO:Declaring metric variables
2024-09-27 17:29:15,311:INFO:Importing untrained model
2024-09-27 17:29:15,314:INFO:Naive Bayes Imported successfully
2024-09-27 17:29:15,321:INFO:Starting cross validation
2024-09-27 17:29:15,322:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-27 17:29:15,324:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.
  warnings.warn(

2024-09-27 17:29:15,387:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 17:29:15,389:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,391:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,391:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,392:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,393:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,393:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,394:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,395:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,395:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,396:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,396:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,396:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,396:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,397:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,397:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,397:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,397:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,398:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 17:29:15,399:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,399:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,400:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,400:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,401:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,401:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,401:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,402:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,405:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,403:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 17:29:15,407:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,409:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,409:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,410:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,411:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,413:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,414:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,415:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,418:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,419:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,433:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 17:29:15,434:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,435:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,436:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,437:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,437:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,437:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 17:29:15,439:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,440:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,440:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,442:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,442:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,444:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,445:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,449:INFO:Calculating mean and std
2024-09-27 17:29:15,450:INFO:Creating metrics dataframe
2024-09-27 17:29:15,451:INFO:Uploading results into container
2024-09-27 17:29:15,452:INFO:Uploading model into container now
2024-09-27 17:29:15,452:INFO:_master_model_container: 3
2024-09-27 17:29:15,452:INFO:_display_container: 2
2024-09-27 17:29:15,452:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-27 17:29:15,452:INFO:create_model() successfully completed......................................
2024-09-27 17:29:15,521:INFO:SubProcess create_model() end ==================================
2024-09-27 17:29:15,521:INFO:Creating metrics dataframe
2024-09-27 17:29:15,527:INFO:Initializing Decision Tree Classifier
2024-09-27 17:29:15,527:INFO:Total runtime is 0.18885269562403362 minutes
2024-09-27 17:29:15,532:INFO:SubProcess create_model() called ==================================
2024-09-27 17:29:15,532:INFO:Initializing create_model()
2024-09-27 17:29:15,532:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EC22FAD0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C1ED9282D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 17:29:15,532:INFO:Checking exceptions
2024-09-27 17:29:15,533:INFO:Importing libraries
2024-09-27 17:29:15,533:INFO:Copying training dataset
2024-09-27 17:29:15,535:INFO:Defining folds
2024-09-27 17:29:15,535:INFO:Declaring metric variables
2024-09-27 17:29:15,537:INFO:Importing untrained model
2024-09-27 17:29:15,540:INFO:Decision Tree Classifier Imported successfully
2024-09-27 17:29:15,546:INFO:Starting cross validation
2024-09-27 17:29:15,547:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-27 17:29:15,550:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.
  warnings.warn(

2024-09-27 17:29:15,614:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 17:29:15,616:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,616:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 17:29:15,617:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,618:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,618:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,618:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,618:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,619:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,619:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,619:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,620:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,620:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,620:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,621:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,621:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,621:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,622:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,622:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,622:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,623:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,623:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,623:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,623:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,624:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,624:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,625:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,625:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

ifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,625:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 17:29:15,626:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,626:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,627:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,627:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,633:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,638:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,640:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,642:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,643:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,668:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 17:29:15,670:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,671:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,672:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,672:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 17:29:15,673:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,673:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,673:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,673:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,674:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,675:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,675:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,676:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,677:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,685:INFO:Calculating mean and std
2024-09-27 17:29:15,687:INFO:Creating metrics dataframe
2024-09-27 17:29:15,688:INFO:Uploading results into container
2024-09-27 17:29:15,689:INFO:Uploading model into container now
2024-09-27 17:29:15,689:INFO:_master_model_container: 4
2024-09-27 17:29:15,689:INFO:_display_container: 2
2024-09-27 17:29:15,689:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-27 17:29:15,690:INFO:create_model() successfully completed......................................
2024-09-27 17:29:15,759:INFO:SubProcess create_model() end ==================================
2024-09-27 17:29:15,760:INFO:Creating metrics dataframe
2024-09-27 17:29:15,767:INFO:Initializing SVM - Linear Kernel
2024-09-27 17:29:15,767:INFO:Total runtime is 0.1928606867790222 minutes
2024-09-27 17:29:15,771:INFO:SubProcess create_model() called ==================================
2024-09-27 17:29:15,772:INFO:Initializing create_model()
2024-09-27 17:29:15,772:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EC22FAD0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C1ED9282D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 17:29:15,772:INFO:Checking exceptions
2024-09-27 17:29:15,772:INFO:Importing libraries
2024-09-27 17:29:15,772:INFO:Copying training dataset
2024-09-27 17:29:15,774:INFO:Defining folds
2024-09-27 17:29:15,774:INFO:Declaring metric variables
2024-09-27 17:29:15,777:INFO:Importing untrained model
2024-09-27 17:29:15,782:INFO:SVM - Linear Kernel Imported successfully
2024-09-27 17:29:15,790:INFO:Starting cross validation
2024-09-27 17:29:15,791:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-27 17:29:15,794:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.
  warnings.warn(

2024-09-27 17:29:15,870:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:15,872:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,872:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:15,873:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:15,873:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:15,873:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:15,874:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:15,874:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,875:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,875:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,876:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,876:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,877:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,877:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,878:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,878:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,879:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,881:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,881:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,882:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,882:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,883:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,883:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,883:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,884:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,885:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,885:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,885:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,887:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,887:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,887:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,899:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:15,901:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,901:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:15,902:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,903:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,906:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,907:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,907:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,908:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,908:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,909:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,910:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,911:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,912:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,941:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:15,941:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:15,943:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,943:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,944:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,944:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,945:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,945:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,946:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,947:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:15,948:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,948:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:15,957:INFO:Calculating mean and std
2024-09-27 17:29:15,958:INFO:Creating metrics dataframe
2024-09-27 17:29:15,960:INFO:Uploading results into container
2024-09-27 17:29:15,961:INFO:Uploading model into container now
2024-09-27 17:29:15,961:INFO:_master_model_container: 5
2024-09-27 17:29:15,961:INFO:_display_container: 2
2024-09-27 17:29:15,962:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-27 17:29:15,962:INFO:create_model() successfully completed......................................
2024-09-27 17:29:16,049:INFO:SubProcess create_model() end ==================================
2024-09-27 17:29:16,051:INFO:Creating metrics dataframe
2024-09-27 17:29:16,060:INFO:Initializing Ridge Classifier
2024-09-27 17:29:16,060:INFO:Total runtime is 0.1977353811264038 minutes
2024-09-27 17:29:16,063:INFO:SubProcess create_model() called ==================================
2024-09-27 17:29:16,063:INFO:Initializing create_model()
2024-09-27 17:29:16,063:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EC22FAD0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C1ED9282D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 17:29:16,063:INFO:Checking exceptions
2024-09-27 17:29:16,064:INFO:Importing libraries
2024-09-27 17:29:16,064:INFO:Copying training dataset
2024-09-27 17:29:16,067:INFO:Defining folds
2024-09-27 17:29:16,067:INFO:Declaring metric variables
2024-09-27 17:29:16,070:INFO:Importing untrained model
2024-09-27 17:29:16,072:INFO:Ridge Classifier Imported successfully
2024-09-27 17:29:16,080:INFO:Starting cross validation
2024-09-27 17:29:16,082:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-27 17:29:16,087:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.
  warnings.warn(

2024-09-27 17:29:16,168:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:16,168:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:16,169:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:16,170:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:16,170:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:16,170:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,170:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,170:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,171:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,172:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,172:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,173:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:16,173:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:16,174:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,174:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,174:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,174:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,174:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,174:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,175:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,175:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,175:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,175:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,175:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,176:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,176:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,177:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,177:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,177:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,177:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,177:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,177:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,178:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,178:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,182:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,183:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,184:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,186:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,186:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,187:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,188:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,189:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,192:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,193:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,215:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:16,216:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,216:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,218:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,218:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,220:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,221:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,221:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,227:INFO:Calculating mean and std
2024-09-27 17:29:16,230:INFO:Creating metrics dataframe
2024-09-27 17:29:16,233:INFO:Uploading results into container
2024-09-27 17:29:16,235:INFO:Uploading model into container now
2024-09-27 17:29:16,235:INFO:_master_model_container: 6
2024-09-27 17:29:16,236:INFO:_display_container: 2
2024-09-27 17:29:16,236:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-27 17:29:16,236:INFO:create_model() successfully completed......................................
2024-09-27 17:29:16,310:INFO:SubProcess create_model() end ==================================
2024-09-27 17:29:16,310:INFO:Creating metrics dataframe
2024-09-27 17:29:16,317:INFO:Initializing Random Forest Classifier
2024-09-27 17:29:16,317:INFO:Total runtime is 0.20202281872431435 minutes
2024-09-27 17:29:16,320:INFO:SubProcess create_model() called ==================================
2024-09-27 17:29:16,321:INFO:Initializing create_model()
2024-09-27 17:29:16,321:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EC22FAD0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C1ED9282D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 17:29:16,321:INFO:Checking exceptions
2024-09-27 17:29:16,321:INFO:Importing libraries
2024-09-27 17:29:16,321:INFO:Copying training dataset
2024-09-27 17:29:16,323:INFO:Defining folds
2024-09-27 17:29:16,324:INFO:Declaring metric variables
2024-09-27 17:29:16,326:INFO:Importing untrained model
2024-09-27 17:29:16,331:INFO:Random Forest Classifier Imported successfully
2024-09-27 17:29:16,339:INFO:Starting cross validation
2024-09-27 17:29:16,340:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-27 17:29:16,343:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.
  warnings.warn(

2024-09-27 17:29:16,720:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 17:29:16,722:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,723:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,724:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,725:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,726:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,727:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,729:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,733:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,736:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,736:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,739:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,740:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,740:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,742:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,742:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,744:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,749:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,749:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,751:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,751:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,754:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 17:29:16,758:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,760:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,762:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,763:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,765:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,766:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,768:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,768:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,770:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,785:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 17:29:16,785:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,787:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,789:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,790:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,791:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,792:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,972:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 17:29:16,975:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,976:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,978:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,982:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,983:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 17:29:16,984:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,985:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,985:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,986:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,987:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,988:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:16,990:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:16,991:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,006:INFO:Calculating mean and std
2024-09-27 17:29:17,006:INFO:Creating metrics dataframe
2024-09-27 17:29:17,009:INFO:Uploading results into container
2024-09-27 17:29:17,010:INFO:Uploading model into container now
2024-09-27 17:29:17,010:INFO:_master_model_container: 7
2024-09-27 17:29:17,010:INFO:_display_container: 2
2024-09-27 17:29:17,011:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-27 17:29:17,011:INFO:create_model() successfully completed......................................
2024-09-27 17:29:17,099:INFO:SubProcess create_model() end ==================================
2024-09-27 17:29:17,100:INFO:Creating metrics dataframe
2024-09-27 17:29:17,110:INFO:Initializing Quadratic Discriminant Analysis
2024-09-27 17:29:17,110:INFO:Total runtime is 0.2152406573295593 minutes
2024-09-27 17:29:17,113:INFO:SubProcess create_model() called ==================================
2024-09-27 17:29:17,114:INFO:Initializing create_model()
2024-09-27 17:29:17,114:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EC22FAD0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C1ED9282D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 17:29:17,114:INFO:Checking exceptions
2024-09-27 17:29:17,114:INFO:Importing libraries
2024-09-27 17:29:17,114:INFO:Copying training dataset
2024-09-27 17:29:17,117:INFO:Defining folds
2024-09-27 17:29:17,118:INFO:Declaring metric variables
2024-09-27 17:29:17,121:INFO:Importing untrained model
2024-09-27 17:29:17,125:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-27 17:29:17,135:INFO:Starting cross validation
2024-09-27 17:29:17,136:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-27 17:29:17,139:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.
  warnings.warn(

2024-09-27 17:29:17,199:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-27 17:29:17,200:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-27 17:29:17,221:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:960: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2024-09-27 17:29:17,221:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:963: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2024-09-27 17:29:17,221:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-27 17:29:17,221:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:964: RuntimeWarning: invalid value encountered in add
  return -0.5 * (norm2 + u) + np.log(self.priors_)

2024-09-27 17:29:17,222:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:960: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2024-09-27 17:29:17,222:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:963: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2024-09-27 17:29:17,222:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:964: RuntimeWarning: invalid value encountered in add
  return -0.5 * (norm2 + u) + np.log(self.priors_)

2024-09-27 17:29:17,226:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-27 17:29:17,228:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:960: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2024-09-27 17:29:17,228:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:960: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2024-09-27 17:29:17,228:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:963: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2024-09-27 17:29:17,228:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:964: RuntimeWarning: invalid value encountered in add
  return -0.5 * (norm2 + u) + np.log(self.priors_)

2024-09-27 17:29:17,228:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:963: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2024-09-27 17:29:17,228:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:964: RuntimeWarning: invalid value encountered in add
  return -0.5 * (norm2 + u) + np.log(self.priors_)

2024-09-27 17:29:17,228:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-27 17:29:17,231:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:960: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2024-09-27 17:29:17,232:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:963: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2024-09-27 17:29:17,232:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:964: RuntimeWarning: invalid value encountered in add
  return -0.5 * (norm2 + u) + np.log(self.priors_)

2024-09-27 17:29:17,233:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 619, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2024-09-27 17:29:17,235:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,235:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,236:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-27 17:29:17,237:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:960: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2024-09-27 17:29:17,238:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:963: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2024-09-27 17:29:17,238:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:960: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2024-09-27 17:29:17,238:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:964: RuntimeWarning: invalid value encountered in add
  return -0.5 * (norm2 + u) + np.log(self.priors_)

2024-09-27 17:29:17,239:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:963: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2024-09-27 17:29:17,239:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:964: RuntimeWarning: invalid value encountered in add
  return -0.5 * (norm2 + u) + np.log(self.priors_)

2024-09-27 17:29:17,241:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,241:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,242:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:960: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2024-09-27 17:29:17,242:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,243:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:963: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2024-09-27 17:29:17,243:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:963: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2024-09-27 17:29:17,243:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:964: RuntimeWarning: invalid value encountered in add
  return -0.5 * (norm2 + u) + np.log(self.priors_)

2024-09-27 17:29:17,243:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:960: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2024-09-27 17:29:17,243:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:964: RuntimeWarning: invalid value encountered in add
  return -0.5 * (norm2 + u) + np.log(self.priors_)

2024-09-27 17:29:17,243:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:963: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2024-09-27 17:29:17,243:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 619, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2024-09-27 17:29:17,244:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:964: RuntimeWarning: invalid value encountered in add
  return -0.5 * (norm2 + u) + np.log(self.priors_)

2024-09-27 17:29:17,243:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,244:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-27 17:29:17,246:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,247:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:960: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2024-09-27 17:29:17,247:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:963: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2024-09-27 17:29:17,247:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:964: RuntimeWarning: invalid value encountered in add
  return -0.5 * (norm2 + u) + np.log(self.priors_)

2024-09-27 17:29:17,247:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 619, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2024-09-27 17:29:17,248:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,248:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,249:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:960: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2024-09-27 17:29:17,249:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:963: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2024-09-27 17:29:17,249:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:964: RuntimeWarning: invalid value encountered in add
  return -0.5 * (norm2 + u) + np.log(self.priors_)

2024-09-27 17:29:17,250:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,250:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:960: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2024-09-27 17:29:17,250:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:963: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2024-09-27 17:29:17,250:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:964: RuntimeWarning: invalid value encountered in add
  return -0.5 * (norm2 + u) + np.log(self.priors_)

2024-09-27 17:29:17,250:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,251:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 619, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2024-09-27 17:29:17,251:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,253:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,253:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 619, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2024-09-27 17:29:17,253:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,253:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:960: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2024-09-27 17:29:17,254:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,254:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:963: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2024-09-27 17:29:17,255:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:964: RuntimeWarning: invalid value encountered in add
  return -0.5 * (norm2 + u) + np.log(self.priors_)

2024-09-27 17:29:17,258:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,258:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,259:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:960: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2024-09-27 17:29:17,260:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:963: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2024-09-27 17:29:17,260:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:964: RuntimeWarning: invalid value encountered in add
  return -0.5 * (norm2 + u) + np.log(self.priors_)

2024-09-27 17:29:17,260:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 619, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2024-09-27 17:29:17,261:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,261:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,262:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,263:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:960: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2024-09-27 17:29:17,263:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,263:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,263:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:963: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2024-09-27 17:29:17,263:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:964: RuntimeWarning: invalid value encountered in add
  return -0.5 * (norm2 + u) + np.log(self.priors_)

2024-09-27 17:29:17,263:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,265:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,267:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,268:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,268:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 619, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2024-09-27 17:29:17,269:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,271:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,271:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,272:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,274:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,274:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,275:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,276:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,277:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,278:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,278:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,279:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,282:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-27 17:29:17,286:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-27 17:29:17,288:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:960: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2024-09-27 17:29:17,288:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:963: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2024-09-27 17:29:17,288:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:964: RuntimeWarning: invalid value encountered in add
  return -0.5 * (norm2 + u) + np.log(self.priors_)

2024-09-27 17:29:17,290:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:960: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2024-09-27 17:29:17,290:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:963: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2024-09-27 17:29:17,290:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:964: RuntimeWarning: invalid value encountered in add
  return -0.5 * (norm2 + u) + np.log(self.priors_)

2024-09-27 17:29:17,290:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:960: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2024-09-27 17:29:17,292:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:963: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2024-09-27 17:29:17,292:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:964: RuntimeWarning: invalid value encountered in add
  return -0.5 * (norm2 + u) + np.log(self.priors_)

2024-09-27 17:29:17,292:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 619, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2024-09-27 17:29:17,293:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,293:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:960: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2024-09-27 17:29:17,293:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:963: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2024-09-27 17:29:17,294:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:964: RuntimeWarning: invalid value encountered in add
  return -0.5 * (norm2 + u) + np.log(self.priors_)

2024-09-27 17:29:17,294:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,295:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 619, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\validation.py", line 1049, in check_array
    _assert_all_finite(
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\validation.py", line 126, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\validation.py", line 175, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2024-09-27 17:29:17,295:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,296:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,297:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,297:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,298:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,298:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,299:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,299:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,300:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,301:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,313:INFO:Calculating mean and std
2024-09-27 17:29:17,313:INFO:Creating metrics dataframe
2024-09-27 17:29:17,316:INFO:Uploading results into container
2024-09-27 17:29:17,317:INFO:Uploading model into container now
2024-09-27 17:29:17,317:INFO:_master_model_container: 8
2024-09-27 17:29:17,317:INFO:_display_container: 2
2024-09-27 17:29:17,317:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-27 17:29:17,317:INFO:create_model() successfully completed......................................
2024-09-27 17:29:17,389:INFO:SubProcess create_model() end ==================================
2024-09-27 17:29:17,389:INFO:Creating metrics dataframe
2024-09-27 17:29:17,397:INFO:Initializing Ada Boost Classifier
2024-09-27 17:29:17,397:INFO:Total runtime is 0.2200289726257324 minutes
2024-09-27 17:29:17,399:INFO:SubProcess create_model() called ==================================
2024-09-27 17:29:17,399:INFO:Initializing create_model()
2024-09-27 17:29:17,400:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EC22FAD0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C1ED9282D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 17:29:17,400:INFO:Checking exceptions
2024-09-27 17:29:17,400:INFO:Importing libraries
2024-09-27 17:29:17,400:INFO:Copying training dataset
2024-09-27 17:29:17,402:INFO:Defining folds
2024-09-27 17:29:17,402:INFO:Declaring metric variables
2024-09-27 17:29:17,408:INFO:Importing untrained model
2024-09-27 17:29:17,412:INFO:Ada Boost Classifier Imported successfully
2024-09-27 17:29:17,419:INFO:Starting cross validation
2024-09-27 17:29:17,420:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-27 17:29:17,423:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.
  warnings.warn(

2024-09-27 17:29:17,478:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-27 17:29:17,478:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-27 17:29:17,479:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-27 17:29:17,480:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-27 17:29:17,483:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-27 17:29:17,488:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-27 17:29:17,495:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-27 17:29:17,499:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-27 17:29:17,629:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:17,632:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,632:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:17,632:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:17,633:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,633:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,634:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,635:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,637:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,637:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,638:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,639:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,639:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,639:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,642:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,643:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,650:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:17,650:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,652:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,658:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,660:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,662:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,669:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:17,670:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:17,671:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:17,671:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,672:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,673:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,673:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,674:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:17,675:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,676:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,676:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,677:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,677:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,677:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,677:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-27 17:29:17,677:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,678:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,679:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,680:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,680:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,681:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,682:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,684:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,687:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,690:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,765:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:17,766:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,767:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,768:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,769:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:17,769:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,770:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,770:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,771:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,772:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,772:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,773:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,775:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:17,777:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:17,786:INFO:Calculating mean and std
2024-09-27 17:29:17,787:INFO:Creating metrics dataframe
2024-09-27 17:29:17,789:INFO:Uploading results into container
2024-09-27 17:29:17,789:INFO:Uploading model into container now
2024-09-27 17:29:17,789:INFO:_master_model_container: 9
2024-09-27 17:29:17,789:INFO:_display_container: 2
2024-09-27 17:29:17,790:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-27 17:29:17,790:INFO:create_model() successfully completed......................................
2024-09-27 17:29:17,869:INFO:SubProcess create_model() end ==================================
2024-09-27 17:29:17,870:INFO:Creating metrics dataframe
2024-09-27 17:29:17,878:INFO:Initializing Gradient Boosting Classifier
2024-09-27 17:29:17,879:INFO:Total runtime is 0.2280535578727722 minutes
2024-09-27 17:29:17,883:INFO:SubProcess create_model() called ==================================
2024-09-27 17:29:17,883:INFO:Initializing create_model()
2024-09-27 17:29:17,884:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EC22FAD0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C1ED9282D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 17:29:17,884:INFO:Checking exceptions
2024-09-27 17:29:17,884:INFO:Importing libraries
2024-09-27 17:29:17,884:INFO:Copying training dataset
2024-09-27 17:29:17,887:INFO:Defining folds
2024-09-27 17:29:17,888:INFO:Declaring metric variables
2024-09-27 17:29:17,893:INFO:Importing untrained model
2024-09-27 17:29:17,899:INFO:Gradient Boosting Classifier Imported successfully
2024-09-27 17:29:17,909:INFO:Starting cross validation
2024-09-27 17:29:17,910:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-27 17:29:17,916:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.
  warnings.warn(

2024-09-27 17:29:18,373:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:18,376:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,377:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,380:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,383:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,386:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,387:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,395:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:18,397:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:18,397:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,401:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,401:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,406:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,407:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:18,408:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,409:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,410:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,411:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,411:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,413:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,413:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,414:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,416:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,418:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,424:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:18,427:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,432:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,434:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,434:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:18,436:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,439:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,440:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,442:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,443:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:18,445:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,448:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,450:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,452:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,459:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:18,460:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,461:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,462:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,463:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,465:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,466:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,620:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:18,621:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,622:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,624:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,625:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,626:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,627:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,629:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:18,632:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,633:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,635:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,636:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,637:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,638:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,645:INFO:Calculating mean and std
2024-09-27 17:29:18,646:INFO:Creating metrics dataframe
2024-09-27 17:29:18,647:INFO:Uploading results into container
2024-09-27 17:29:18,648:INFO:Uploading model into container now
2024-09-27 17:29:18,648:INFO:_master_model_container: 10
2024-09-27 17:29:18,648:INFO:_display_container: 2
2024-09-27 17:29:18,648:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-27 17:29:18,648:INFO:create_model() successfully completed......................................
2024-09-27 17:29:18,718:INFO:SubProcess create_model() end ==================================
2024-09-27 17:29:18,718:INFO:Creating metrics dataframe
2024-09-27 17:29:18,725:INFO:Initializing Linear Discriminant Analysis
2024-09-27 17:29:18,725:INFO:Total runtime is 0.24216158787409464 minutes
2024-09-27 17:29:18,729:INFO:SubProcess create_model() called ==================================
2024-09-27 17:29:18,730:INFO:Initializing create_model()
2024-09-27 17:29:18,730:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EC22FAD0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C1ED9282D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 17:29:18,730:INFO:Checking exceptions
2024-09-27 17:29:18,730:INFO:Importing libraries
2024-09-27 17:29:18,730:INFO:Copying training dataset
2024-09-27 17:29:18,733:INFO:Defining folds
2024-09-27 17:29:18,733:INFO:Declaring metric variables
2024-09-27 17:29:18,735:INFO:Importing untrained model
2024-09-27 17:29:18,738:INFO:Linear Discriminant Analysis Imported successfully
2024-09-27 17:29:18,744:INFO:Starting cross validation
2024-09-27 17:29:18,745:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-27 17:29:18,747:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.
  warnings.warn(

2024-09-27 17:29:18,809:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:18,810:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:18,811:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,812:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:18,812:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:18,813:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,814:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,814:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,815:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,816:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,817:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,817:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,818:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,818:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,819:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,820:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,819:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:18,820:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,821:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:18,822:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,822:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,823:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,823:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,824:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

ifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,826:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,830:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,834:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,834:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,836:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,836:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,837:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:18,838:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,838:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,839:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,841:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,843:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,844:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:18,845:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,845:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,846:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,849:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,851:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,855:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,858:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,865:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:18,866:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,866:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 17:29:18,867:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,868:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,869:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,869:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,870:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,870:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,872:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,872:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,873:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,873:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:18,874:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:18,882:INFO:Calculating mean and std
2024-09-27 17:29:18,883:INFO:Creating metrics dataframe
2024-09-27 17:29:18,885:INFO:Uploading results into container
2024-09-27 17:29:18,885:INFO:Uploading model into container now
2024-09-27 17:29:18,885:INFO:_master_model_container: 11
2024-09-27 17:29:18,885:INFO:_display_container: 2
2024-09-27 17:29:18,886:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-27 17:29:18,886:INFO:create_model() successfully completed......................................
2024-09-27 17:29:18,961:INFO:SubProcess create_model() end ==================================
2024-09-27 17:29:18,961:INFO:Creating metrics dataframe
2024-09-27 17:29:18,969:INFO:Initializing Extra Trees Classifier
2024-09-27 17:29:18,969:INFO:Total runtime is 0.24622613191604614 minutes
2024-09-27 17:29:18,972:INFO:SubProcess create_model() called ==================================
2024-09-27 17:29:18,972:INFO:Initializing create_model()
2024-09-27 17:29:18,972:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EC22FAD0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C1ED9282D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 17:29:18,973:INFO:Checking exceptions
2024-09-27 17:29:18,973:INFO:Importing libraries
2024-09-27 17:29:18,973:INFO:Copying training dataset
2024-09-27 17:29:18,975:INFO:Defining folds
2024-09-27 17:29:18,975:INFO:Declaring metric variables
2024-09-27 17:29:18,977:INFO:Importing untrained model
2024-09-27 17:29:18,982:INFO:Extra Trees Classifier Imported successfully
2024-09-27 17:29:18,989:INFO:Starting cross validation
2024-09-27 17:29:18,990:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-27 17:29:18,993:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.
  warnings.warn(

2024-09-27 17:29:19,288:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:19,292:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:19,293:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:19,295:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:19,301:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 17:29:19,302:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:19,304:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:19,306:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:19,307:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:19,309:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:19,309:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:19,310:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:19,310:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:19,311:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:19,312:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:19,312:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:19,313:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:19,313:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:19,314:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:19,314:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:19,314:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:19,315:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:19,315:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:19,317:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:19,317:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:19,320:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:19,378:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 17:29:19,379:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:19,382:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:19,384:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 17:29:19,385:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:19,385:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:19,385:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:19,386:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:19,387:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:19,387:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:19,388:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:19,390:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:19,391:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:19,496:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 17:29:19,497:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:19,498:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:19,502:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:19,503:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 17:29:19,503:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:19,504:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:19,505:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:19,506:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:19,507:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:19,508:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:19,509:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:19,510:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:19,511:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:19,515:INFO:Calculating mean and std
2024-09-27 17:29:19,516:INFO:Creating metrics dataframe
2024-09-27 17:29:19,518:INFO:Uploading results into container
2024-09-27 17:29:19,518:INFO:Uploading model into container now
2024-09-27 17:29:19,518:INFO:_master_model_container: 12
2024-09-27 17:29:19,519:INFO:_display_container: 2
2024-09-27 17:29:19,519:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-27 17:29:19,519:INFO:create_model() successfully completed......................................
2024-09-27 17:29:19,597:INFO:SubProcess create_model() end ==================================
2024-09-27 17:29:19,598:INFO:Creating metrics dataframe
2024-09-27 17:29:19,608:INFO:Initializing Light Gradient Boosting Machine
2024-09-27 17:29:19,609:INFO:Total runtime is 0.2568872769673665 minutes
2024-09-27 17:29:19,614:INFO:SubProcess create_model() called ==================================
2024-09-27 17:29:19,614:INFO:Initializing create_model()
2024-09-27 17:29:19,614:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EC22FAD0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C1ED9282D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 17:29:19,614:INFO:Checking exceptions
2024-09-27 17:29:19,615:INFO:Importing libraries
2024-09-27 17:29:19,615:INFO:Copying training dataset
2024-09-27 17:29:19,617:INFO:Defining folds
2024-09-27 17:29:19,617:INFO:Declaring metric variables
2024-09-27 17:29:19,622:INFO:Importing untrained model
2024-09-27 17:29:19,626:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-27 17:29:19,636:INFO:Starting cross validation
2024-09-27 17:29:19,638:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-27 17:29:19,642:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.
  warnings.warn(

2024-09-27 17:29:20,184:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,187:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,188:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,192:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,192:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,194:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,194:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,196:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,198:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,198:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,199:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,199:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,200:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,201:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,202:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,206:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,223:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,226:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,230:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,233:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,250:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 17:29:20,252:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,253:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,257:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,259:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,261:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,263:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,264:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 17:29:20,265:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 17:29:20,265:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,267:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,267:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,269:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,269:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,271:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,271:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,273:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,273:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,275:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,276:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,277:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,367:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 17:29:20,369:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 17:29:20,369:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,370:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,370:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,372:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,372:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,374:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,375:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,375:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,377:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,377:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,378:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,391:INFO:Calculating mean and std
2024-09-27 17:29:20,393:INFO:Creating metrics dataframe
2024-09-27 17:29:20,395:INFO:Uploading results into container
2024-09-27 17:29:20,395:INFO:Uploading model into container now
2024-09-27 17:29:20,396:INFO:_master_model_container: 13
2024-09-27 17:29:20,396:INFO:_display_container: 2
2024-09-27 17:29:20,397:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-27 17:29:20,397:INFO:create_model() successfully completed......................................
2024-09-27 17:29:20,486:INFO:SubProcess create_model() end ==================================
2024-09-27 17:29:20,486:INFO:Creating metrics dataframe
2024-09-27 17:29:20,496:INFO:Initializing Dummy Classifier
2024-09-27 17:29:20,496:INFO:Total runtime is 0.2716785669326782 minutes
2024-09-27 17:29:20,499:INFO:SubProcess create_model() called ==================================
2024-09-27 17:29:20,499:INFO:Initializing create_model()
2024-09-27 17:29:20,500:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EC22FAD0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C1ED9282D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 17:29:20,500:INFO:Checking exceptions
2024-09-27 17:29:20,500:INFO:Importing libraries
2024-09-27 17:29:20,500:INFO:Copying training dataset
2024-09-27 17:29:20,503:INFO:Defining folds
2024-09-27 17:29:20,503:INFO:Declaring metric variables
2024-09-27 17:29:20,508:INFO:Importing untrained model
2024-09-27 17:29:20,511:INFO:Dummy Classifier Imported successfully
2024-09-27 17:29:20,519:INFO:Starting cross validation
2024-09-27 17:29:20,520:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-27 17:29:20,524:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.
  warnings.warn(

2024-09-27 17:29:20,571:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 17:29:20,575:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,577:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,578:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,579:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,581:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,581:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,583:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,583:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,584:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,585:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,585:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,585:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,586:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,586:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,588:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,588:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,589:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,590:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 17:29:20,590:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,591:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,592:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,592:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 17:29:20,593:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,594:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,594:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,594:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,596:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,596:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,596:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,599:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,599:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,600:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,601:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,602:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,603:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,603:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,607:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,622:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 17:29:20,623:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 17:29:20,623:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,624:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,624:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,625:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,625:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,626:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,626:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,627:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,628:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,628:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 17:29:20,628:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,631:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 17:29:20,640:INFO:Calculating mean and std
2024-09-27 17:29:20,640:INFO:Creating metrics dataframe
2024-09-27 17:29:20,642:INFO:Uploading results into container
2024-09-27 17:29:20,642:INFO:Uploading model into container now
2024-09-27 17:29:20,643:INFO:_master_model_container: 14
2024-09-27 17:29:20,643:INFO:_display_container: 2
2024-09-27 17:29:20,643:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-27 17:29:20,643:INFO:create_model() successfully completed......................................
2024-09-27 17:29:20,726:INFO:SubProcess create_model() end ==================================
2024-09-27 17:29:20,726:INFO:Creating metrics dataframe
2024-09-27 17:29:20,740:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-09-27 17:29:20,750:INFO:Initializing create_model()
2024-09-27 17:29:20,750:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EC22FAD0>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 17:29:20,751:INFO:Checking exceptions
2024-09-27 17:29:20,753:INFO:Importing libraries
2024-09-27 17:29:20,753:INFO:Copying training dataset
2024-09-27 17:29:20,758:INFO:Defining folds
2024-09-27 17:29:20,759:INFO:Declaring metric variables
2024-09-27 17:29:20,759:INFO:Importing untrained model
2024-09-27 17:29:20,759:INFO:Declaring custom model
2024-09-27 17:29:20,760:INFO:Logistic Regression Imported successfully
2024-09-27 17:29:20,762:INFO:Cross validation set to False
2024-09-27 17:29:20,762:INFO:Fitting Model
2024-09-27 17:29:20,803:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-27 17:29:20,806:INFO:create_model() successfully completed......................................
2024-09-27 17:29:20,899:INFO:Initializing create_model()
2024-09-27 17:29:20,900:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EC22FAD0>, estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 17:29:20,900:INFO:Checking exceptions
2024-09-27 17:29:20,901:INFO:Importing libraries
2024-09-27 17:29:20,901:INFO:Copying training dataset
2024-09-27 17:29:20,903:INFO:Defining folds
2024-09-27 17:29:20,903:INFO:Declaring metric variables
2024-09-27 17:29:20,903:INFO:Importing untrained model
2024-09-27 17:29:20,903:INFO:Declaring custom model
2024-09-27 17:29:20,904:INFO:K Neighbors Classifier Imported successfully
2024-09-27 17:29:20,905:INFO:Cross validation set to False
2024-09-27 17:29:20,905:INFO:Fitting Model
2024-09-27 17:29:20,922:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-27 17:29:20,922:INFO:create_model() successfully completed......................................
2024-09-27 17:29:21,020:INFO:Initializing create_model()
2024-09-27 17:29:21,020:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EC22FAD0>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 17:29:21,020:INFO:Checking exceptions
2024-09-27 17:29:21,022:INFO:Importing libraries
2024-09-27 17:29:21,022:INFO:Copying training dataset
2024-09-27 17:29:21,025:INFO:Defining folds
2024-09-27 17:29:21,025:INFO:Declaring metric variables
2024-09-27 17:29:21,025:INFO:Importing untrained model
2024-09-27 17:29:21,025:INFO:Declaring custom model
2024-09-27 17:29:21,026:INFO:Naive Bayes Imported successfully
2024-09-27 17:29:21,026:INFO:Cross validation set to False
2024-09-27 17:29:21,027:INFO:Fitting Model
2024-09-27 17:29:21,044:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-27 17:29:21,044:INFO:create_model() successfully completed......................................
2024-09-27 17:29:21,142:INFO:_master_model_container: 14
2024-09-27 17:29:21,142:INFO:_display_container: 2
2024-09-27 17:29:21,142:INFO:[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), GaussianNB(priors=None, var_smoothing=1e-09)]
2024-09-27 17:29:21,143:INFO:compare_models() successfully completed......................................
2024-09-27 18:14:00,756:INFO:PyCaret ClassificationExperiment
2024-09-27 18:14:00,758:INFO:Logging name: clf-default-name
2024-09-27 18:14:00,758:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-27 18:14:00,758:INFO:version 3.3.2
2024-09-27 18:14:00,758:INFO:Initializing setup()
2024-09-27 18:14:00,758:INFO:self.USI: b9d6
2024-09-27 18:14:00,758:INFO:self._variable_keys: {'fix_imbalance', 'gpu_param', 'pipeline', 'X', 'is_multiclass', 'log_plots_param', 'seed', 'X_train', 'gpu_n_jobs_param', 'data', 'y_test', 'exp_id', 'y', 'html_param', 'exp_name_log', '_ml_usecase', 'X_test', 'n_jobs_param', '_available_plots', 'USI', 'fold_generator', 'fold_groups_param', 'idx', 'target_param', 'logging_param', 'fold_shuffle_param', 'memory', 'y_train'}
2024-09-27 18:14:00,761:INFO:Checking environment
2024-09-27 18:14:00,762:INFO:python_version: 3.11.9
2024-09-27 18:14:00,762:INFO:python_build: ('tags/v3.11.9:de54cf5', 'Apr  2 2024 10:12:12')
2024-09-27 18:14:00,763:INFO:machine: AMD64
2024-09-27 18:14:00,763:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-27 18:14:00,774:INFO:Memory: svmem(total=8434716672, available=851677184, percent=89.9, used=7583039488, free=851677184)
2024-09-27 18:14:00,774:INFO:Physical Core: 4
2024-09-27 18:14:00,775:INFO:Logical Core: 8
2024-09-27 18:14:00,775:INFO:Checking libraries
2024-09-27 18:14:00,776:INFO:System:
2024-09-27 18:14:00,777:INFO:    python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]
2024-09-27 18:14:00,777:INFO:executable: d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Scripts\python.exe
2024-09-27 18:14:00,777:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-27 18:14:00,777:INFO:PyCaret required dependencies:
2024-09-27 18:14:00,786:INFO:                 pip: 24.0
2024-09-27 18:14:00,786:INFO:          setuptools: 65.5.0
2024-09-27 18:14:00,786:INFO:             pycaret: 3.3.2
2024-09-27 18:14:00,786:INFO:             IPython: 8.27.0
2024-09-27 18:14:00,786:INFO:          ipywidgets: 8.1.5
2024-09-27 18:14:00,786:INFO:                tqdm: 4.66.5
2024-09-27 18:14:00,786:INFO:               numpy: 1.26.4
2024-09-27 18:14:00,786:INFO:              pandas: 2.1.4
2024-09-27 18:14:00,786:INFO:              jinja2: 3.1.4
2024-09-27 18:14:00,786:INFO:               scipy: 1.11.4
2024-09-27 18:14:00,786:INFO:              joblib: 1.3.2
2024-09-27 18:14:00,786:INFO:             sklearn: 1.4.2
2024-09-27 18:14:00,786:INFO:                pyod: 2.0.2
2024-09-27 18:14:00,786:INFO:            imblearn: 0.12.3
2024-09-27 18:14:00,786:INFO:   category_encoders: 2.6.3
2024-09-27 18:14:00,786:INFO:            lightgbm: 4.5.0
2024-09-27 18:14:00,786:INFO:               numba: 0.60.0
2024-09-27 18:14:00,786:INFO:            requests: 2.32.3
2024-09-27 18:14:00,786:INFO:          matplotlib: 3.7.5
2024-09-27 18:14:00,786:INFO:          scikitplot: 0.3.7
2024-09-27 18:14:00,787:INFO:         yellowbrick: 1.5
2024-09-27 18:14:00,787:INFO:              plotly: 5.24.1
2024-09-27 18:14:00,787:INFO:    plotly-resampler: Not installed
2024-09-27 18:14:00,787:INFO:             kaleido: 0.2.1
2024-09-27 18:14:00,787:INFO:           schemdraw: 0.15
2024-09-27 18:14:00,787:INFO:         statsmodels: 0.14.3
2024-09-27 18:14:00,787:INFO:              sktime: 0.26.0
2024-09-27 18:14:00,787:INFO:               tbats: 1.1.3
2024-09-27 18:14:00,787:INFO:            pmdarima: 2.0.4
2024-09-27 18:14:00,787:INFO:              psutil: 6.0.0
2024-09-27 18:14:00,787:INFO:          markupsafe: 2.1.5
2024-09-27 18:14:00,787:INFO:             pickle5: Not installed
2024-09-27 18:14:00,787:INFO:         cloudpickle: 3.0.0
2024-09-27 18:14:00,787:INFO:         deprecation: 2.1.0
2024-09-27 18:14:00,787:INFO:              xxhash: 3.5.0
2024-09-27 18:14:00,787:INFO:           wurlitzer: Not installed
2024-09-27 18:14:00,787:INFO:PyCaret optional dependencies:
2024-09-27 18:14:00,788:INFO:                shap: Not installed
2024-09-27 18:14:00,788:INFO:           interpret: Not installed
2024-09-27 18:14:00,788:INFO:                umap: Not installed
2024-09-27 18:14:00,788:INFO:     ydata_profiling: Not installed
2024-09-27 18:14:00,788:INFO:  explainerdashboard: Not installed
2024-09-27 18:14:00,788:INFO:             autoviz: Not installed
2024-09-27 18:14:00,788:INFO:           fairlearn: Not installed
2024-09-27 18:14:00,788:INFO:          deepchecks: Not installed
2024-09-27 18:14:00,788:INFO:             xgboost: Not installed
2024-09-27 18:14:00,788:INFO:            catboost: Not installed
2024-09-27 18:14:00,788:INFO:              kmodes: Not installed
2024-09-27 18:14:00,788:INFO:             mlxtend: Not installed
2024-09-27 18:14:00,788:INFO:       statsforecast: Not installed
2024-09-27 18:14:00,788:INFO:        tune_sklearn: Not installed
2024-09-27 18:14:00,788:INFO:                 ray: Not installed
2024-09-27 18:14:00,788:INFO:            hyperopt: Not installed
2024-09-27 18:14:00,789:INFO:              optuna: Not installed
2024-09-27 18:14:00,789:INFO:               skopt: Not installed
2024-09-27 18:14:00,789:INFO:              mlflow: Not installed
2024-09-27 18:14:00,789:INFO:              gradio: Not installed
2024-09-27 18:14:00,789:INFO:             fastapi: Not installed
2024-09-27 18:14:00,789:INFO:             uvicorn: Not installed
2024-09-27 18:14:00,789:INFO:              m2cgen: Not installed
2024-09-27 18:14:00,789:INFO:           evidently: Not installed
2024-09-27 18:14:00,789:INFO:               fugue: Not installed
2024-09-27 18:14:00,789:INFO:           streamlit: Not installed
2024-09-27 18:14:00,789:INFO:             prophet: Not installed
2024-09-27 18:14:00,789:INFO:None
2024-09-27 18:14:00,793:INFO:Set up data.
2024-09-27 18:14:03,172:INFO:Set up folding strategy.
2024-09-27 18:14:03,172:INFO:Set up train/test split.
2024-09-27 18:14:03,858:INFO:Set up index.
2024-09-27 18:14:03,866:INFO:Assigning column types.
2024-09-27 18:14:04,686:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-27 18:14:04,732:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-27 18:14:04,733:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-27 18:14:04,760:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-27 18:14:04,763:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-27 18:14:04,806:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-27 18:14:04,806:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-27 18:14:04,833:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-27 18:14:04,833:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-27 18:14:04,834:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-27 18:14:04,877:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-27 18:14:04,902:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-27 18:14:04,903:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-27 18:14:04,945:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-27 18:14:04,974:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-27 18:14:04,974:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-27 18:14:04,974:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-27 18:14:05,049:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-27 18:14:05,049:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-27 18:14:05,117:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-27 18:14:05,118:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-27 18:14:05,121:INFO:Preparing preprocessing pipeline...
2024-09-27 18:14:05,191:INFO:Set up label encoding.
2024-09-27 18:14:05,191:INFO:Set up simple imputation.
2024-09-27 18:14:05,255:INFO:Set up column name cleaning.
2024-09-27 18:14:07,016:INFO:Finished creating preprocessing pipeline.
2024-09-27 18:14:07,041:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ASUS\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['aback', 'ability', 'able',
                                             'aboutrestaurant', 'abruptness',
                                             'absence', 'absolute',
                                             'absoluteall', 'absolutel...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-27 18:14:07,041:INFO:Creating final display dataframe.
2024-09-27 18:14:09,990:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\pipeline.py:289: UserWarning: Persisting input arguments took 0.96s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  X, y = self._memory_full_transform(

2024-09-27 18:14:12,804:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\pipeline.py:289: UserWarning: Persisting input arguments took 1.00s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  X, y = self._memory_full_transform(

2024-09-27 18:14:15,031:INFO:Setup _display_container:                     Description                                 Value
0                    Session id                                   123
1                        Target                       sentiment_final
2                   Target type                            Multiclass
3                Target mapping  Negative: 0, Neutral: 1, Positive: 2
4           Original data shape                           (195, 4265)
5        Transformed data shape                           (195, 4265)
6   Transformed train set shape                           (136, 4265)
7    Transformed test set shape                            (59, 4265)
8              Numeric features                                  4264
9                    Preprocess                                  True
10              Imputation type                                simple
11           Numeric imputation                                  mean
12       Categorical imputation                                  mode
13               Fold Generator                       StratifiedKFold
14                  Fold Number                                    10
15                     CPU Jobs                                    -1
16                      Use GPU                                 False
17               Log Experiment                                 False
18              Experiment Name                      clf-default-name
19                          USI                                  b9d6
2024-09-27 18:14:15,116:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-27 18:14:15,116:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-27 18:14:15,187:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-27 18:14:15,187:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-27 18:14:15,189:INFO:setup() successfully completed in 14.52s...............
2024-09-27 18:14:23,300:INFO:Initializing compare_models()
2024-09-27 18:14:23,300:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EE7BD150>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EE7BD150>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-27 18:14:23,301:INFO:Checking exceptions
2024-09-27 18:14:23,909:INFO:Preparing display monitor
2024-09-27 18:14:23,929:INFO:Initializing Logistic Regression
2024-09-27 18:14:23,929:INFO:Total runtime is 0.0 minutes
2024-09-27 18:14:23,933:INFO:SubProcess create_model() called ==================================
2024-09-27 18:14:23,934:INFO:Initializing create_model()
2024-09-27 18:14:23,934:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EE7BD150>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C1EBC87850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 18:14:23,934:INFO:Checking exceptions
2024-09-27 18:14:23,934:INFO:Importing libraries
2024-09-27 18:14:23,935:INFO:Copying training dataset
2024-09-27 18:14:24,733:INFO:Defining folds
2024-09-27 18:14:24,733:INFO:Declaring metric variables
2024-09-27 18:14:24,736:INFO:Importing untrained model
2024-09-27 18:14:24,739:INFO:Logistic Regression Imported successfully
2024-09-27 18:14:24,745:INFO:Starting cross validation
2024-09-27 18:14:24,758:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-27 18:14:24,768:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.
  warnings.warn(

2024-09-27 18:14:35,243:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:14:35,245:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:14:35,247:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:35,249:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:35,254:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:35,256:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:35,257:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:14:35,257:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:14:35,258:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:35,260:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:35,260:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:35,263:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:35,263:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:35,265:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:35,265:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:35,267:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:35,268:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:35,268:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:35,272:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:35,283:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:14:35,289:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:35,289:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:14:35,290:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:35,291:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:35,293:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:35,293:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:35,295:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:35,295:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:35,297:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:35,297:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:35,298:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:35,301:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:14:35,301:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:35,305:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:35,307:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:35,308:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:35,310:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:35,312:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:35,313:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:35,315:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:35,301:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:35,319:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:35,320:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:35,322:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:35,325:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:36,152:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:14:36,154:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:36,155:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:36,156:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:36,157:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:36,158:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:36,159:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:36,257:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:14:36,260:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:36,262:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:36,264:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:36,265:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:36,267:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:36,268:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:36,281:INFO:Calculating mean and std
2024-09-27 18:14:36,284:INFO:Creating metrics dataframe
2024-09-27 18:14:36,287:INFO:Uploading results into container
2024-09-27 18:14:36,287:INFO:Uploading model into container now
2024-09-27 18:14:36,288:INFO:_master_model_container: 1
2024-09-27 18:14:36,288:INFO:_display_container: 2
2024-09-27 18:14:36,289:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-27 18:14:36,289:INFO:create_model() successfully completed......................................
2024-09-27 18:14:36,396:INFO:SubProcess create_model() end ==================================
2024-09-27 18:14:36,396:INFO:Creating metrics dataframe
2024-09-27 18:14:36,408:INFO:Initializing K Neighbors Classifier
2024-09-27 18:14:36,408:INFO:Total runtime is 0.20798182884852093 minutes
2024-09-27 18:14:36,412:INFO:SubProcess create_model() called ==================================
2024-09-27 18:14:36,412:INFO:Initializing create_model()
2024-09-27 18:14:36,412:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EE7BD150>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C1EBC87850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 18:14:36,412:INFO:Checking exceptions
2024-09-27 18:14:36,412:INFO:Importing libraries
2024-09-27 18:14:36,412:INFO:Copying training dataset
2024-09-27 18:14:37,190:INFO:Defining folds
2024-09-27 18:14:37,190:INFO:Declaring metric variables
2024-09-27 18:14:37,194:INFO:Importing untrained model
2024-09-27 18:14:37,196:INFO:K Neighbors Classifier Imported successfully
2024-09-27 18:14:37,204:INFO:Starting cross validation
2024-09-27 18:14:37,222:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-27 18:14:37,230:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.
  warnings.warn(

2024-09-27 18:14:38,223:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:14:38,232:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:38,234:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:38,237:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:38,238:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:38,241:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:38,243:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:38,423:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:38,433:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:38,434:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:38,437:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:38,693:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:38,703:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:38,710:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:38,723:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:38,912:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:38,915:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:38,917:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:38,919:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:39,085:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:39,088:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:39,089:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:39,091:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:39,247:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:39,253:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:39,256:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:39,259:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:39,707:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:14:39,709:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:39,710:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:39,711:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:39,713:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:39,714:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:39,715:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:39,787:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:14:39,788:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:39,789:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:39,791:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:39,792:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:39,793:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:39,794:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:39,816:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:14:39,817:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:39,818:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:39,820:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:39,821:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:39,822:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:39,823:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:39,915:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:14:39,916:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:39,917:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:39,918:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:39,919:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:39,920:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:39,922:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:39,931:INFO:Calculating mean and std
2024-09-27 18:14:39,932:INFO:Creating metrics dataframe
2024-09-27 18:14:39,934:INFO:Uploading results into container
2024-09-27 18:14:39,935:INFO:Uploading model into container now
2024-09-27 18:14:39,935:INFO:_master_model_container: 2
2024-09-27 18:14:39,935:INFO:_display_container: 2
2024-09-27 18:14:39,936:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-27 18:14:39,936:INFO:create_model() successfully completed......................................
2024-09-27 18:14:40,034:INFO:SubProcess create_model() end ==================================
2024-09-27 18:14:40,034:INFO:Creating metrics dataframe
2024-09-27 18:14:40,047:INFO:Initializing Naive Bayes
2024-09-27 18:14:40,047:INFO:Total runtime is 0.2686306953430176 minutes
2024-09-27 18:14:40,050:INFO:SubProcess create_model() called ==================================
2024-09-27 18:14:40,050:INFO:Initializing create_model()
2024-09-27 18:14:40,050:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EE7BD150>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C1EBC87850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 18:14:40,050:INFO:Checking exceptions
2024-09-27 18:14:40,050:INFO:Importing libraries
2024-09-27 18:14:40,051:INFO:Copying training dataset
2024-09-27 18:14:40,746:INFO:Defining folds
2024-09-27 18:14:40,746:INFO:Declaring metric variables
2024-09-27 18:14:40,750:INFO:Importing untrained model
2024-09-27 18:14:40,755:INFO:Naive Bayes Imported successfully
2024-09-27 18:14:40,760:INFO:Starting cross validation
2024-09-27 18:14:40,773:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-27 18:14:40,778:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.
  warnings.warn(

2024-09-27 18:14:41,742:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:14:41,743:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:41,745:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:41,747:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:41,749:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:41,754:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:41,757:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:41,909:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:41,912:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:41,914:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:41,916:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:42,069:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:42,073:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:42,075:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:42,077:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:42,218:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:42,223:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:42,226:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:42,229:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:42,489:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:42,492:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:42,495:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:42,499:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:42,642:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:42,646:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:42,648:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:42,651:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:42,820:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:14:42,821:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:42,824:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:42,827:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:42,831:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:42,832:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:42,834:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:42,952:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:14:42,954:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:42,956:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:42,959:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:42,960:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:42,962:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:42,964:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:43,061:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:14:43,062:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:43,063:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:43,064:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:43,065:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:43,066:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:43,067:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:43,160:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:14:43,161:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:43,162:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:43,163:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:43,164:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:43,165:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:43,167:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:43,178:INFO:Calculating mean and std
2024-09-27 18:14:43,181:INFO:Creating metrics dataframe
2024-09-27 18:14:43,182:INFO:Uploading results into container
2024-09-27 18:14:43,183:INFO:Uploading model into container now
2024-09-27 18:14:43,184:INFO:_master_model_container: 3
2024-09-27 18:14:43,184:INFO:_display_container: 2
2024-09-27 18:14:43,184:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-27 18:14:43,184:INFO:create_model() successfully completed......................................
2024-09-27 18:14:43,299:INFO:SubProcess create_model() end ==================================
2024-09-27 18:14:43,299:INFO:Creating metrics dataframe
2024-09-27 18:14:43,311:INFO:Initializing Decision Tree Classifier
2024-09-27 18:14:43,311:INFO:Total runtime is 0.3230237245559692 minutes
2024-09-27 18:14:43,315:INFO:SubProcess create_model() called ==================================
2024-09-27 18:14:43,315:INFO:Initializing create_model()
2024-09-27 18:14:43,316:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EE7BD150>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C1EBC87850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 18:14:43,316:INFO:Checking exceptions
2024-09-27 18:14:43,316:INFO:Importing libraries
2024-09-27 18:14:43,316:INFO:Copying training dataset
2024-09-27 18:14:44,036:INFO:Defining folds
2024-09-27 18:14:44,036:INFO:Declaring metric variables
2024-09-27 18:14:44,040:INFO:Importing untrained model
2024-09-27 18:14:44,042:INFO:Decision Tree Classifier Imported successfully
2024-09-27 18:14:44,047:INFO:Starting cross validation
2024-09-27 18:14:44,061:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-27 18:14:44,066:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.
  warnings.warn(

2024-09-27 18:14:44,899:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:14:44,905:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:44,907:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:44,909:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:44,911:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:44,914:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:44,916:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:45,077:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:45,082:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:45,083:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:45,085:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:45,243:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:45,247:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:45,249:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:45,252:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:45,386:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:45,390:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:45,391:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:45,394:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:45,613:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:45,617:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:45,620:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:45,621:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:45,783:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:45,786:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:45,789:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:45,791:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:46,106:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:14:46,108:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:46,109:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:46,110:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:46,112:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:46,114:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:46,116:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:46,199:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:14:46,201:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:46,202:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:46,205:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:46,207:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:46,208:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:46,211:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:46,323:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:14:46,324:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:46,325:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:46,327:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:46,330:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:46,429:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:14:46,432:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:46,434:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:46,436:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:46,437:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:46,439:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:46,441:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:46,454:INFO:Calculating mean and std
2024-09-27 18:14:46,458:INFO:Creating metrics dataframe
2024-09-27 18:14:46,462:INFO:Uploading results into container
2024-09-27 18:14:46,463:INFO:Uploading model into container now
2024-09-27 18:14:46,464:INFO:_master_model_container: 4
2024-09-27 18:14:46,464:INFO:_display_container: 2
2024-09-27 18:14:46,464:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-27 18:14:46,465:INFO:create_model() successfully completed......................................
2024-09-27 18:14:46,638:INFO:SubProcess create_model() end ==================================
2024-09-27 18:14:46,638:INFO:Creating metrics dataframe
2024-09-27 18:14:46,649:INFO:Initializing SVM - Linear Kernel
2024-09-27 18:14:46,649:INFO:Total runtime is 0.37865912516911826 minutes
2024-09-27 18:14:46,656:INFO:SubProcess create_model() called ==================================
2024-09-27 18:14:46,656:INFO:Initializing create_model()
2024-09-27 18:14:46,656:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EE7BD150>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C1EBC87850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 18:14:46,656:INFO:Checking exceptions
2024-09-27 18:14:46,656:INFO:Importing libraries
2024-09-27 18:14:46,656:INFO:Copying training dataset
2024-09-27 18:14:47,572:INFO:Defining folds
2024-09-27 18:14:47,572:INFO:Declaring metric variables
2024-09-27 18:14:47,575:INFO:Importing untrained model
2024-09-27 18:14:47,580:INFO:SVM - Linear Kernel Imported successfully
2024-09-27 18:14:47,589:INFO:Starting cross validation
2024-09-27 18:14:47,615:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-27 18:14:47,619:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.
  warnings.warn(

2024-09-27 18:14:48,876:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:14:48,880:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:48,881:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:48,884:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:48,887:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:48,893:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:48,896:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:49,106:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:14:49,110:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:49,121:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:49,175:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:49,184:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:49,546:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:14:49,549:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:49,553:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:49,556:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:49,562:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:49,805:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:14:49,807:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:49,817:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:49,821:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:49,823:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:50,018:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:14:50,020:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:50,024:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:50,026:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:50,031:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:50,202:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:14:50,205:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:50,209:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:50,215:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:50,220:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:50,647:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:14:50,649:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:50,651:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:50,655:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:50,657:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:50,659:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:50,665:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:50,771:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:14:50,774:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:50,775:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:50,776:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:50,777:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:50,779:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:50,781:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:50,801:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:14:50,805:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:50,807:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:50,809:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:50,812:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:50,816:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:50,817:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:50,949:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:14:50,951:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:50,952:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:50,954:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:50,955:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:50,957:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:50,958:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:50,971:INFO:Calculating mean and std
2024-09-27 18:14:50,972:INFO:Creating metrics dataframe
2024-09-27 18:14:50,973:INFO:Uploading results into container
2024-09-27 18:14:50,974:INFO:Uploading model into container now
2024-09-27 18:14:50,974:INFO:_master_model_container: 5
2024-09-27 18:14:50,974:INFO:_display_container: 2
2024-09-27 18:14:50,975:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-27 18:14:50,975:INFO:create_model() successfully completed......................................
2024-09-27 18:14:51,102:INFO:SubProcess create_model() end ==================================
2024-09-27 18:14:51,102:INFO:Creating metrics dataframe
2024-09-27 18:14:51,116:INFO:Initializing Ridge Classifier
2024-09-27 18:14:51,117:INFO:Total runtime is 0.45312889019648234 minutes
2024-09-27 18:14:51,121:INFO:SubProcess create_model() called ==================================
2024-09-27 18:14:51,121:INFO:Initializing create_model()
2024-09-27 18:14:51,121:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EE7BD150>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C1EBC87850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 18:14:51,122:INFO:Checking exceptions
2024-09-27 18:14:51,122:INFO:Importing libraries
2024-09-27 18:14:51,122:INFO:Copying training dataset
2024-09-27 18:14:52,031:INFO:Defining folds
2024-09-27 18:14:52,031:INFO:Declaring metric variables
2024-09-27 18:14:52,038:INFO:Importing untrained model
2024-09-27 18:14:52,043:INFO:Ridge Classifier Imported successfully
2024-09-27 18:14:52,047:INFO:Starting cross validation
2024-09-27 18:14:52,071:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-27 18:14:52,076:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.
  warnings.warn(

2024-09-27 18:14:53,098:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:14:53,100:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:53,102:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:53,105:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:53,110:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:53,117:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:53,119:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:53,347:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:14:53,349:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:53,355:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:53,357:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:53,359:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:53,541:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:14:53,544:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:53,547:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:53,548:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:53,550:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:53,750:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:14:53,752:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:53,758:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:53,759:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:53,760:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:53,961:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:14:53,965:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:53,969:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:53,970:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:53,973:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:54,171:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:14:54,173:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:54,177:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:54,182:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:54,185:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:54,598:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:14:54,601:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:54,603:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:54,607:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:54,609:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:54,612:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:54,614:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:54,747:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:14:54,748:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:54,749:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:54,750:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:54,753:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:54,756:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:54,758:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:54,862:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:14:54,863:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:54,865:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:54,867:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:54,868:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:54,870:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:54,871:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:54,991:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:14:54,992:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:54,993:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:54,995:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:54,997:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:54,998:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:54,999:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:55,012:INFO:Calculating mean and std
2024-09-27 18:14:55,014:INFO:Creating metrics dataframe
2024-09-27 18:14:55,016:INFO:Uploading results into container
2024-09-27 18:14:55,016:INFO:Uploading model into container now
2024-09-27 18:14:55,016:INFO:_master_model_container: 6
2024-09-27 18:14:55,017:INFO:_display_container: 2
2024-09-27 18:14:55,017:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-27 18:14:55,017:INFO:create_model() successfully completed......................................
2024-09-27 18:14:55,127:INFO:SubProcess create_model() end ==================================
2024-09-27 18:14:55,127:INFO:Creating metrics dataframe
2024-09-27 18:14:55,139:INFO:Initializing Random Forest Classifier
2024-09-27 18:14:55,139:INFO:Total runtime is 0.5201535026232401 minutes
2024-09-27 18:14:55,142:INFO:SubProcess create_model() called ==================================
2024-09-27 18:14:55,142:INFO:Initializing create_model()
2024-09-27 18:14:55,142:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EE7BD150>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C1EBC87850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 18:14:55,142:INFO:Checking exceptions
2024-09-27 18:14:55,142:INFO:Importing libraries
2024-09-27 18:14:55,142:INFO:Copying training dataset
2024-09-27 18:14:56,074:INFO:Defining folds
2024-09-27 18:14:56,075:INFO:Declaring metric variables
2024-09-27 18:14:56,078:INFO:Importing untrained model
2024-09-27 18:14:56,082:INFO:Random Forest Classifier Imported successfully
2024-09-27 18:14:56,093:INFO:Starting cross validation
2024-09-27 18:14:56,111:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-27 18:14:56,117:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.
  warnings.warn(

2024-09-27 18:14:58,368:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:14:58,371:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:58,373:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:58,375:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:58,377:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:58,382:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:58,384:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:59,194:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:59,198:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:59,201:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:59,203:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:59,447:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:59,457:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:59,459:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:59,462:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:59,621:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:59,625:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:59,628:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:59,636:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:59,740:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:59,745:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:14:59,746:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:14:59,748:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:00,389:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:00,395:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:00,397:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:00,399:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:00,656:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:15:00,658:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:00,660:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:00,663:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:00,664:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:00,666:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:00,667:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:00,810:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:15:00,812:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:00,816:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:00,818:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:00,819:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:00,820:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:00,821:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:00,932:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:15:00,934:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:00,935:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:00,936:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:00,938:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:00,941:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:00,942:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:01,038:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:15:01,039:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:01,040:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:01,041:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:01,042:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:01,043:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:01,044:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:01,068:INFO:Calculating mean and std
2024-09-27 18:15:01,070:INFO:Creating metrics dataframe
2024-09-27 18:15:01,072:INFO:Uploading results into container
2024-09-27 18:15:01,073:INFO:Uploading model into container now
2024-09-27 18:15:01,073:INFO:_master_model_container: 7
2024-09-27 18:15:01,073:INFO:_display_container: 2
2024-09-27 18:15:01,073:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-27 18:15:01,074:INFO:create_model() successfully completed......................................
2024-09-27 18:15:01,187:INFO:SubProcess create_model() end ==================================
2024-09-27 18:15:01,187:INFO:Creating metrics dataframe
2024-09-27 18:15:01,193:INFO:Initializing Quadratic Discriminant Analysis
2024-09-27 18:15:01,193:INFO:Total runtime is 0.6210654060045878 minutes
2024-09-27 18:15:01,196:INFO:SubProcess create_model() called ==================================
2024-09-27 18:15:01,196:INFO:Initializing create_model()
2024-09-27 18:15:01,196:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EE7BD150>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C1EBC87850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 18:15:01,196:INFO:Checking exceptions
2024-09-27 18:15:01,196:INFO:Importing libraries
2024-09-27 18:15:01,196:INFO:Copying training dataset
2024-09-27 18:15:02,133:INFO:Defining folds
2024-09-27 18:15:02,133:INFO:Declaring metric variables
2024-09-27 18:15:02,137:INFO:Importing untrained model
2024-09-27 18:15:02,144:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-27 18:15:02,152:INFO:Starting cross validation
2024-09-27 18:15:02,172:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-27 18:15:02,178:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.
  warnings.warn(

2024-09-27 18:15:03,310:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-27 18:15:03,649:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-27 18:15:04,202:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-27 18:15:04,296:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:04,302:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:04,307:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:04,313:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:04,322:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:04,499:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-27 18:15:04,761:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-27 18:15:04,812:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:04,814:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:04,820:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:04,826:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:04,912:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:04,915:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:04,921:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:04,926:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:05,044:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:05,046:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:05,049:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:05,052:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:05,071:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-27 18:15:05,208:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:05,213:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:05,220:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:05,226:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:05,572:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:05,583:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:05,586:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-27 18:15:05,589:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:05,597:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:05,894:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-27 18:15:06,168:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-27 18:15:06,184:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:06,188:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:06,190:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:06,193:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:06,198:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:06,335:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:06,337:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:06,339:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:06,341:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:06,342:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-27 18:15:06,346:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:06,505:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:06,507:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:06,508:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:06,511:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:06,515:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:06,638:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:06,640:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:06,641:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:06,643:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:06,645:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:06,663:INFO:Calculating mean and std
2024-09-27 18:15:06,664:INFO:Creating metrics dataframe
2024-09-27 18:15:06,666:INFO:Uploading results into container
2024-09-27 18:15:06,667:INFO:Uploading model into container now
2024-09-27 18:15:06,667:INFO:_master_model_container: 8
2024-09-27 18:15:06,668:INFO:_display_container: 2
2024-09-27 18:15:06,668:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-27 18:15:06,668:INFO:create_model() successfully completed......................................
2024-09-27 18:15:06,788:INFO:SubProcess create_model() end ==================================
2024-09-27 18:15:06,788:INFO:Creating metrics dataframe
2024-09-27 18:15:06,796:INFO:Initializing Ada Boost Classifier
2024-09-27 18:15:06,796:INFO:Total runtime is 0.7144492904345194 minutes
2024-09-27 18:15:06,798:INFO:SubProcess create_model() called ==================================
2024-09-27 18:15:06,800:INFO:Initializing create_model()
2024-09-27 18:15:06,800:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EE7BD150>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C1EBC87850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 18:15:06,800:INFO:Checking exceptions
2024-09-27 18:15:06,800:INFO:Importing libraries
2024-09-27 18:15:06,800:INFO:Copying training dataset
2024-09-27 18:15:07,700:INFO:Defining folds
2024-09-27 18:15:07,701:INFO:Declaring metric variables
2024-09-27 18:15:07,710:INFO:Importing untrained model
2024-09-27 18:15:07,716:INFO:Ada Boost Classifier Imported successfully
2024-09-27 18:15:07,722:INFO:Starting cross validation
2024-09-27 18:15:07,742:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-27 18:15:07,748:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.
  warnings.warn(

2024-09-27 18:15:08,649:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-27 18:15:08,924:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-27 18:15:09,336:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-27 18:15:09,779:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-27 18:15:10,097:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-27 18:15:10,774:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:10,777:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:10,778:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:10,786:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:10,791:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:10,994:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:10,997:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:11,003:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:11,005:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:11,013:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:11,204:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-27 18:15:11,760:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:11,769:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:11,774:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:11,777:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:11,788:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:11,902:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-27 18:15:11,916:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:11,923:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:11,928:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:11,936:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:12,077:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:12,079:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:12,090:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:12,092:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:12,099:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:12,215:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-27 18:15:12,487:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-27 18:15:12,693:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-27 18:15:12,902:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:12,920:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:12,927:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:12,930:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:12,942:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:13,320:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:13,322:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:13,324:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:13,327:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:13,330:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:13,332:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:13,338:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:13,465:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:13,468:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:13,470:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:13,472:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:13,473:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:13,475:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:13,478:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:13,640:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:13,641:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:13,642:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:13,643:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:13,644:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:13,645:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:13,646:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:13,709:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:13,710:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:13,711:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:13,713:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:13,714:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:13,715:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:13,716:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:13,727:INFO:Calculating mean and std
2024-09-27 18:15:13,728:INFO:Creating metrics dataframe
2024-09-27 18:15:13,731:INFO:Uploading results into container
2024-09-27 18:15:13,731:INFO:Uploading model into container now
2024-09-27 18:15:13,732:INFO:_master_model_container: 9
2024-09-27 18:15:13,732:INFO:_display_container: 2
2024-09-27 18:15:13,733:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-27 18:15:13,733:INFO:create_model() successfully completed......................................
2024-09-27 18:15:13,830:INFO:SubProcess create_model() end ==================================
2024-09-27 18:15:13,830:INFO:Creating metrics dataframe
2024-09-27 18:15:14,203:INFO:Initializing Gradient Boosting Classifier
2024-09-27 18:15:14,203:INFO:Total runtime is 0.8378999869028727 minutes
2024-09-27 18:15:14,206:INFO:SubProcess create_model() called ==================================
2024-09-27 18:15:14,206:INFO:Initializing create_model()
2024-09-27 18:15:14,207:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EE7BD150>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C1EBC87850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 18:15:14,207:INFO:Checking exceptions
2024-09-27 18:15:14,207:INFO:Importing libraries
2024-09-27 18:15:14,207:INFO:Copying training dataset
2024-09-27 18:15:14,884:INFO:Defining folds
2024-09-27 18:15:14,884:INFO:Declaring metric variables
2024-09-27 18:15:14,887:INFO:Importing untrained model
2024-09-27 18:15:14,889:INFO:Gradient Boosting Classifier Imported successfully
2024-09-27 18:15:14,894:INFO:Starting cross validation
2024-09-27 18:15:14,908:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-27 18:15:14,927:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.
  warnings.warn(

2024-09-27 18:15:23,543:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:23,545:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:23,547:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:23,547:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:23,547:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:23,549:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:23,549:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:23,549:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:23,551:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:23,551:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:23,552:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:23,553:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:23,553:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:23,554:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:23,555:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:23,556:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:23,556:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:23,556:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:23,558:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:23,558:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:23,559:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:23,563:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:23,687:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:23,689:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:23,692:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:23,693:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:23,695:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:23,832:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:23,837:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:23,842:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:23,843:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:23,845:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:23,972:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:23,974:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:23,975:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:23,978:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:23,979:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:23,980:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:23,981:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:24,198:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:24,325:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:24,326:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:24,328:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:24,328:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:24,330:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:24,331:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:27,627:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:27,628:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:27,629:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:27,630:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:27,630:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:27,632:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:27,633:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:27,702:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:27,703:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:27,704:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:27,705:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:27,706:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:27,707:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:27,708:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:27,721:INFO:Calculating mean and std
2024-09-27 18:15:27,722:INFO:Creating metrics dataframe
2024-09-27 18:15:27,723:INFO:Uploading results into container
2024-09-27 18:15:27,724:INFO:Uploading model into container now
2024-09-27 18:15:27,724:INFO:_master_model_container: 10
2024-09-27 18:15:27,724:INFO:_display_container: 2
2024-09-27 18:15:27,725:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-27 18:15:27,725:INFO:create_model() successfully completed......................................
2024-09-27 18:15:27,808:INFO:SubProcess create_model() end ==================================
2024-09-27 18:15:27,808:INFO:Creating metrics dataframe
2024-09-27 18:15:28,001:INFO:Initializing Linear Discriminant Analysis
2024-09-27 18:15:28,002:INFO:Total runtime is 1.0678516149520874 minutes
2024-09-27 18:15:28,006:INFO:SubProcess create_model() called ==================================
2024-09-27 18:15:28,006:INFO:Initializing create_model()
2024-09-27 18:15:28,007:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EE7BD150>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C1EBC87850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 18:15:28,007:INFO:Checking exceptions
2024-09-27 18:15:28,007:INFO:Importing libraries
2024-09-27 18:15:28,007:INFO:Copying training dataset
2024-09-27 18:15:28,726:INFO:Defining folds
2024-09-27 18:15:28,726:INFO:Declaring metric variables
2024-09-27 18:15:28,730:INFO:Importing untrained model
2024-09-27 18:15:28,734:INFO:Linear Discriminant Analysis Imported successfully
2024-09-27 18:15:28,741:INFO:Starting cross validation
2024-09-27 18:15:28,754:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-27 18:15:28,758:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.
  warnings.warn(

2024-09-27 18:15:34,046:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:34,048:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:34,052:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:34,056:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:34,058:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:34,085:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:34,090:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:34,093:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:34,096:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:34,096:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:34,097:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:34,098:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:34,099:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:34,101:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:34,102:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:34,104:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:34,106:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:34,155:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:34,157:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:34,163:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:34,168:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:34,171:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:34,192:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:34,194:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:34,196:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:34,199:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:34,201:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:34,205:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:34,207:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:34,221:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:34,223:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:34,227:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:34,228:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:34,230:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:34,247:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:34,249:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:34,252:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:34,253:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:34,254:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:34,254:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:34,256:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:34,256:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:34,258:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:34,260:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:34,267:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:34,269:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:34,978:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:34,980:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:34,982:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:34,983:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:34,985:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:34,987:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:34,989:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:35,085:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-27 18:15:35,089:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:35,090:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:35,093:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:35,096:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:35,098:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:35,099:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:35,115:INFO:Calculating mean and std
2024-09-27 18:15:35,117:INFO:Creating metrics dataframe
2024-09-27 18:15:35,119:INFO:Uploading results into container
2024-09-27 18:15:35,119:INFO:Uploading model into container now
2024-09-27 18:15:35,120:INFO:_master_model_container: 11
2024-09-27 18:15:35,120:INFO:_display_container: 2
2024-09-27 18:15:35,120:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-27 18:15:35,120:INFO:create_model() successfully completed......................................
2024-09-27 18:15:35,235:INFO:SubProcess create_model() end ==================================
2024-09-27 18:15:35,235:INFO:Creating metrics dataframe
2024-09-27 18:15:35,249:INFO:Initializing Extra Trees Classifier
2024-09-27 18:15:35,249:INFO:Total runtime is 1.1886510610580443 minutes
2024-09-27 18:15:35,252:INFO:SubProcess create_model() called ==================================
2024-09-27 18:15:35,252:INFO:Initializing create_model()
2024-09-27 18:15:35,252:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EE7BD150>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C1EBC87850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 18:15:35,252:INFO:Checking exceptions
2024-09-27 18:15:35,252:INFO:Importing libraries
2024-09-27 18:15:35,253:INFO:Copying training dataset
2024-09-27 18:15:35,988:INFO:Defining folds
2024-09-27 18:15:35,990:INFO:Declaring metric variables
2024-09-27 18:15:35,993:INFO:Importing untrained model
2024-09-27 18:15:35,996:INFO:Extra Trees Classifier Imported successfully
2024-09-27 18:15:36,000:INFO:Starting cross validation
2024-09-27 18:15:36,013:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-27 18:15:36,019:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.
  warnings.warn(

2024-09-27 18:15:37,599:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:15:37,606:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:37,611:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:37,618:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:37,620:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:37,623:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:37,625:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:38,106:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:38,111:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:38,113:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:38,117:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:38,227:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:38,233:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:38,234:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:38,237:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:38,504:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:38,509:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:38,511:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:38,516:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:38,572:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:38,579:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:38,581:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:38,584:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:38,859:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:38,863:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:38,865:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:38,867:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:39,078:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:15:39,081:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:39,083:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:39,085:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:39,087:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:39,091:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:39,093:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:39,269:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:15:39,272:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:39,274:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:39,276:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:39,278:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:39,280:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:39,282:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:39,397:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:15:39,398:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:39,399:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:39,400:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:39,402:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:39,403:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:39,404:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:39,532:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:15:39,533:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:39,534:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:39,535:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:39,536:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:39,540:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:39,541:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:39,557:INFO:Calculating mean and std
2024-09-27 18:15:39,558:INFO:Creating metrics dataframe
2024-09-27 18:15:39,560:INFO:Uploading results into container
2024-09-27 18:15:39,560:INFO:Uploading model into container now
2024-09-27 18:15:39,561:INFO:_master_model_container: 12
2024-09-27 18:15:39,561:INFO:_display_container: 2
2024-09-27 18:15:39,562:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-27 18:15:39,562:INFO:create_model() successfully completed......................................
2024-09-27 18:15:39,679:INFO:SubProcess create_model() end ==================================
2024-09-27 18:15:39,679:INFO:Creating metrics dataframe
2024-09-27 18:15:39,710:INFO:Initializing Light Gradient Boosting Machine
2024-09-27 18:15:39,710:INFO:Total runtime is 1.2630098303159076 minutes
2024-09-27 18:15:39,715:INFO:SubProcess create_model() called ==================================
2024-09-27 18:15:39,716:INFO:Initializing create_model()
2024-09-27 18:15:39,716:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EE7BD150>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C1EBC87850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 18:15:39,716:INFO:Checking exceptions
2024-09-27 18:15:39,716:INFO:Importing libraries
2024-09-27 18:15:39,716:INFO:Copying training dataset
2024-09-27 18:15:40,750:INFO:Defining folds
2024-09-27 18:15:40,750:INFO:Declaring metric variables
2024-09-27 18:15:40,759:INFO:Importing untrained model
2024-09-27 18:15:40,763:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-27 18:15:40,770:INFO:Starting cross validation
2024-09-27 18:15:40,785:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-27 18:15:40,791:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.
  warnings.warn(

2024-09-27 18:15:44,826:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:15:44,828:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:44,830:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:44,833:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:44,834:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:44,834:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:15:44,836:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:44,837:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:44,840:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:44,840:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:44,843:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:44,845:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:44,848:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:44,849:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:45,032:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:15:45,035:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:45,037:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:45,045:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:45,048:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:45,049:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:45,052:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:45,054:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:45,057:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:45,628:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:45,633:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:45,636:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:45,645:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:45,681:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:45,696:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:45,698:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:45,710:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:46,324:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:46,329:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:46,330:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:46,334:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:46,602:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:46,607:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:46,609:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:46,612:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:47,007:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:15:47,010:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:47,011:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:47,014:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:47,017:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:47,020:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:47,021:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:47,143:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:15:47,145:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:47,147:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:47,149:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:47,150:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:47,152:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:47,154:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:47,169:INFO:Calculating mean and std
2024-09-27 18:15:47,171:INFO:Creating metrics dataframe
2024-09-27 18:15:47,173:INFO:Uploading results into container
2024-09-27 18:15:47,174:INFO:Uploading model into container now
2024-09-27 18:15:47,175:INFO:_master_model_container: 13
2024-09-27 18:15:47,175:INFO:_display_container: 2
2024-09-27 18:15:47,176:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-27 18:15:47,176:INFO:create_model() successfully completed......................................
2024-09-27 18:15:47,295:INFO:SubProcess create_model() end ==================================
2024-09-27 18:15:47,295:INFO:Creating metrics dataframe
2024-09-27 18:15:47,305:INFO:Initializing Dummy Classifier
2024-09-27 18:15:47,305:INFO:Total runtime is 1.389590295155843 minutes
2024-09-27 18:15:47,309:INFO:SubProcess create_model() called ==================================
2024-09-27 18:15:47,309:INFO:Initializing create_model()
2024-09-27 18:15:47,309:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EE7BD150>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C1EBC87850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 18:15:47,309:INFO:Checking exceptions
2024-09-27 18:15:47,309:INFO:Importing libraries
2024-09-27 18:15:47,309:INFO:Copying training dataset
2024-09-27 18:15:48,194:INFO:Defining folds
2024-09-27 18:15:48,194:INFO:Declaring metric variables
2024-09-27 18:15:48,198:INFO:Importing untrained model
2024-09-27 18:15:48,201:INFO:Dummy Classifier Imported successfully
2024-09-27 18:15:48,208:INFO:Starting cross validation
2024-09-27 18:15:48,226:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-27 18:15:48,231:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.
  warnings.warn(

2024-09-27 18:15:49,343:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:15:49,346:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:49,348:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:49,350:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:49,354:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:49,356:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:49,358:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:49,588:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:49,597:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:49,599:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:49,602:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:49,842:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:49,848:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:49,850:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:49,856:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:50,069:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:50,074:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:50,077:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:50,080:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:50,310:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:50,318:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:50,321:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:50,324:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:50,531:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:50,536:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:50,537:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:50,542:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:50,753:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:15:50,755:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:50,757:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:50,759:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:50,761:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:50,763:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:50,764:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:50,903:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:15:50,905:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:50,906:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:50,909:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:50,909:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:50,911:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:50,912:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:51,086:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:15:51,088:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:51,090:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:51,093:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:51,094:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:51,096:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:51,097:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:51,193:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:15:51,196:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:51,198:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:51,200:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:51,203:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:51,206:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:15:51,207:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:15:51,233:INFO:Calculating mean and std
2024-09-27 18:15:51,234:INFO:Creating metrics dataframe
2024-09-27 18:15:51,236:INFO:Uploading results into container
2024-09-27 18:15:51,237:INFO:Uploading model into container now
2024-09-27 18:15:51,237:INFO:_master_model_container: 14
2024-09-27 18:15:51,237:INFO:_display_container: 2
2024-09-27 18:15:51,238:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-27 18:15:51,238:INFO:create_model() successfully completed......................................
2024-09-27 18:15:51,346:INFO:SubProcess create_model() end ==================================
2024-09-27 18:15:51,346:INFO:Creating metrics dataframe
2024-09-27 18:15:51,357:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-09-27 18:15:51,366:INFO:Initializing create_model()
2024-09-27 18:15:51,366:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EE7BD150>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 18:15:51,367:INFO:Checking exceptions
2024-09-27 18:15:51,369:INFO:Importing libraries
2024-09-27 18:15:51,369:INFO:Copying training dataset
2024-09-27 18:15:52,124:INFO:Defining folds
2024-09-27 18:15:52,124:INFO:Declaring metric variables
2024-09-27 18:15:52,124:INFO:Importing untrained model
2024-09-27 18:15:52,124:INFO:Declaring custom model
2024-09-27 18:15:52,125:INFO:Gradient Boosting Classifier Imported successfully
2024-09-27 18:15:52,138:INFO:Cross validation set to False
2024-09-27 18:15:52,138:INFO:Fitting Model
2024-09-27 18:15:56,375:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-27 18:15:56,375:INFO:create_model() successfully completed......................................
2024-09-27 18:15:56,529:INFO:Initializing create_model()
2024-09-27 18:15:56,529:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EE7BD150>, estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 18:15:56,529:INFO:Checking exceptions
2024-09-27 18:15:56,530:INFO:Importing libraries
2024-09-27 18:15:56,530:INFO:Copying training dataset
2024-09-27 18:15:57,276:INFO:Defining folds
2024-09-27 18:15:57,276:INFO:Declaring metric variables
2024-09-27 18:15:57,276:INFO:Importing untrained model
2024-09-27 18:15:57,276:INFO:Declaring custom model
2024-09-27 18:15:57,277:INFO:K Neighbors Classifier Imported successfully
2024-09-27 18:15:57,288:INFO:Cross validation set to False
2024-09-27 18:15:57,288:INFO:Fitting Model
2024-09-27 18:15:57,523:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-27 18:15:57,523:INFO:create_model() successfully completed......................................
2024-09-27 18:15:57,627:INFO:Initializing create_model()
2024-09-27 18:15:57,627:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EE7BD150>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 18:15:57,628:INFO:Checking exceptions
2024-09-27 18:15:57,629:INFO:Importing libraries
2024-09-27 18:15:57,629:INFO:Copying training dataset
2024-09-27 18:15:58,357:INFO:Defining folds
2024-09-27 18:15:58,357:INFO:Declaring metric variables
2024-09-27 18:15:58,357:INFO:Importing untrained model
2024-09-27 18:15:58,358:INFO:Declaring custom model
2024-09-27 18:15:58,358:INFO:Logistic Regression Imported successfully
2024-09-27 18:15:58,377:INFO:Cross validation set to False
2024-09-27 18:15:58,377:INFO:Fitting Model
2024-09-27 18:15:58,728:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-27 18:15:58,728:INFO:create_model() successfully completed......................................
2024-09-27 18:15:58,851:INFO:_master_model_container: 14
2024-09-27 18:15:58,851:INFO:_display_container: 2
2024-09-27 18:15:58,852:INFO:[GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)]
2024-09-27 18:15:58,852:INFO:compare_models() successfully completed......................................
2024-09-27 18:27:06,116:INFO:Initializing create_model()
2024-09-27 18:27:06,118:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EE7BD150>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 18:27:06,118:INFO:Checking exceptions
2024-09-27 18:27:06,286:INFO:Importing libraries
2024-09-27 18:27:06,288:INFO:Copying training dataset
2024-09-27 18:27:07,650:INFO:Defining folds
2024-09-27 18:27:07,650:INFO:Declaring metric variables
2024-09-27 18:27:07,655:INFO:Importing untrained model
2024-09-27 18:27:07,658:INFO:Random Forest Classifier Imported successfully
2024-09-27 18:27:07,665:INFO:Starting cross validation
2024-09-27 18:27:07,684:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-27 18:27:07,694:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.
  warnings.warn(

2024-09-27 18:27:17,459:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:27:17,464:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:27:17,468:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:27:17,471:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:27:17,473:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:27:17,475:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:27:17,478:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:27:17,484:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:27:17,489:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:27:17,491:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:27:17,494:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:27:17,498:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:27:17,500:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:27:17,507:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:27:17,511:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:27:17,513:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:27:17,515:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:27:17,517:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:27:17,517:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:27:17,521:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:27:17,526:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:27:17,528:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:27:17,535:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:27:17,885:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:27:17,888:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:27:17,889:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:27:17,891:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:27:17,954:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:27:17,958:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:27:17,961:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:27:17,963:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:27:17,965:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:27:17,967:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:27:17,968:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:27:18,019:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:27:18,023:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:27:18,026:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:27:18,028:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:27:18,031:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:27:18,034:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:27:18,036:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:27:18,606:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:27:18,608:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:27:18,609:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:27:18,611:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:27:18,613:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:27:18,615:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:27:18,616:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:27:18,682:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:27:18,685:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:27:18,686:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:27:18,687:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:27:18,687:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:27:18,689:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:27:18,690:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:27:18,704:INFO:Calculating mean and std
2024-09-27 18:27:18,709:INFO:Creating metrics dataframe
2024-09-27 18:27:18,716:INFO:Finalizing model
2024-09-27 18:27:19,068:INFO:Uploading results into container
2024-09-27 18:27:19,070:INFO:Uploading model into container now
2024-09-27 18:27:19,083:INFO:_master_model_container: 15
2024-09-27 18:27:19,084:INFO:_display_container: 3
2024-09-27 18:27:19,084:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-27 18:27:19,085:INFO:create_model() successfully completed......................................
2024-09-27 18:29:41,030:INFO:Initializing create_model()
2024-09-27 18:29:41,030:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EE7BD150>, estimator=rf, fold=None, round=2, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 18:29:41,030:INFO:Checking exceptions
2024-09-27 18:29:41,431:INFO:Importing libraries
2024-09-27 18:29:41,431:INFO:Copying training dataset
2024-09-27 18:29:43,213:INFO:Defining folds
2024-09-27 18:29:43,213:INFO:Declaring metric variables
2024-09-27 18:29:43,216:INFO:Importing untrained model
2024-09-27 18:29:43,219:INFO:Random Forest Classifier Imported successfully
2024-09-27 18:29:43,223:INFO:Starting cross validation
2024-09-27 18:29:43,236:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-27 18:29:43,375:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.
  warnings.warn(

2024-09-27 18:29:45,039:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:29:45,047:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:29:45,048:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:29:45,051:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:29:45,053:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:29:45,056:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:29:45,059:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:29:45,062:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:29:45,063:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:29:45,066:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:29:45,068:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:29:45,735:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:29:45,743:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:29:45,746:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:29:45,749:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:29:47,254:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:29:47,259:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:29:47,266:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:29:47,268:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:29:47,559:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:29:47,563:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:29:47,565:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:29:47,568:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:29:47,733:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:29:47,736:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:29:47,738:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:29:47,740:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:29:47,795:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:29:47,800:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:29:47,802:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:29:47,804:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:29:47,806:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:29:47,806:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:29:47,808:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:29:47,809:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:29:47,810:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:29:47,814:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:29:47,816:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:29:47,818:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:29:47,821:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:29:47,823:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:29:48,203:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:29:48,204:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:29:48,205:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:29:48,206:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:29:48,208:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:29:48,210:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:29:48,213:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:29:48,290:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:29:48,294:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:29:48,295:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:29:48,296:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:29:48,298:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:29:48,300:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:29:48,300:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:29:48,316:INFO:Calculating mean and std
2024-09-27 18:29:48,319:INFO:Creating metrics dataframe
2024-09-27 18:29:48,326:INFO:Finalizing model
2024-09-27 18:29:48,669:INFO:Uploading results into container
2024-09-27 18:29:48,669:INFO:Uploading model into container now
2024-09-27 18:29:48,679:INFO:_master_model_container: 16
2024-09-27 18:29:48,680:INFO:_display_container: 4
2024-09-27 18:29:48,680:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-27 18:29:48,680:INFO:create_model() successfully completed......................................
2024-09-27 18:31:45,650:INFO:Initializing tune_model()
2024-09-27 18:31:45,651:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EE7BD150>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2024-09-27 18:31:45,651:INFO:Checking exceptions
2024-09-27 18:31:46,013:INFO:Copying training dataset
2024-09-27 18:31:46,440:INFO:Checking base model
2024-09-27 18:31:46,440:INFO:Base model : Random Forest Classifier
2024-09-27 18:31:46,444:INFO:Declaring metric variables
2024-09-27 18:31:46,449:INFO:Defining Hyperparameters
2024-09-27 18:31:46,559:INFO:Tuning with n_jobs=-1
2024-09-27 18:31:46,559:INFO:Initializing RandomizedSearchCV
2024-09-27 18:31:46,573:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.
  warnings.warn(

2024-09-27 18:32:26,676:INFO:best_params: {'actual_estimator__n_estimators': 70, 'actual_estimator__min_samples_split': 9, 'actual_estimator__min_samples_leaf': 4, 'actual_estimator__min_impurity_decrease': 0.1, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 3, 'actual_estimator__criterion': 'gini', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': False}
2024-09-27 18:32:26,678:INFO:Hyperparameter search completed
2024-09-27 18:32:26,678:INFO:SubProcess create_model() called ==================================
2024-09-27 18:32:26,680:INFO:Initializing create_model()
2024-09-27 18:32:26,680:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EE7BD150>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C1E806F9D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 70, 'min_samples_split': 9, 'min_samples_leaf': 4, 'min_impurity_decrease': 0.1, 'max_features': 'sqrt', 'max_depth': 3, 'criterion': 'gini', 'class_weight': {}, 'bootstrap': False})
2024-09-27 18:32:26,680:INFO:Checking exceptions
2024-09-27 18:32:26,680:INFO:Importing libraries
2024-09-27 18:32:26,680:INFO:Copying training dataset
2024-09-27 18:32:27,578:INFO:Defining folds
2024-09-27 18:32:27,579:INFO:Declaring metric variables
2024-09-27 18:32:27,582:INFO:Importing untrained model
2024-09-27 18:32:27,582:INFO:Declaring custom model
2024-09-27 18:32:27,589:INFO:Random Forest Classifier Imported successfully
2024-09-27 18:32:27,595:INFO:Starting cross validation
2024-09-27 18:32:27,609:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-27 18:32:27,613:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.
  warnings.warn(

2024-09-27 18:32:28,716:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:32:28,720:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:28,722:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:28,726:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:28,730:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:28,733:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:28,735:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:28,984:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:28,989:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:28,992:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:28,996:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:29,647:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:29,653:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:29,655:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:29,658:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:29,953:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:29,960:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:29,962:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:29,966:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:30,114:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:30,121:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:30,123:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:30,125:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:30,380:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:30,385:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:30,387:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:30,389:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:30,428:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:32:30,431:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:30,434:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:30,437:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:30,439:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:30,443:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:30,446:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:30,648:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:32:30,650:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:30,651:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:30,652:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:30,653:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:30,655:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:30,656:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:30,713:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:32:30,714:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:30,719:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:30,720:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:30,722:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:30,724:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:30,726:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:30,828:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:32:30,829:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:30,830:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:30,831:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:30,831:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:30,834:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:30,835:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:30,854:INFO:Calculating mean and std
2024-09-27 18:32:30,855:INFO:Creating metrics dataframe
2024-09-27 18:32:30,860:INFO:Finalizing model
2024-09-27 18:32:31,086:INFO:Uploading results into container
2024-09-27 18:32:31,086:INFO:Uploading model into container now
2024-09-27 18:32:31,088:INFO:_master_model_container: 17
2024-09-27 18:32:31,088:INFO:_display_container: 5
2024-09-27 18:32:31,088:INFO:RandomForestClassifier(bootstrap=False, ccp_alpha=0.0, class_weight={},
                       criterion='gini', max_depth=3, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.1, min_samples_leaf=4,
                       min_samples_split=9, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=70, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-27 18:32:31,088:INFO:create_model() successfully completed......................................
2024-09-27 18:32:31,205:INFO:SubProcess create_model() end ==================================
2024-09-27 18:32:31,208:INFO:choose_better activated
2024-09-27 18:32:31,211:INFO:SubProcess create_model() called ==================================
2024-09-27 18:32:31,211:INFO:Initializing create_model()
2024-09-27 18:32:31,211:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EE7BD150>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-27 18:32:31,212:INFO:Checking exceptions
2024-09-27 18:32:31,213:INFO:Importing libraries
2024-09-27 18:32:31,213:INFO:Copying training dataset
2024-09-27 18:32:31,949:INFO:Defining folds
2024-09-27 18:32:31,950:INFO:Declaring metric variables
2024-09-27 18:32:31,950:INFO:Importing untrained model
2024-09-27 18:32:31,950:INFO:Declaring custom model
2024-09-27 18:32:31,950:INFO:Random Forest Classifier Imported successfully
2024-09-27 18:32:31,950:INFO:Starting cross validation
2024-09-27 18:32:31,962:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-27 18:32:31,968:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.
  warnings.warn(

2024-09-27 18:32:33,413:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:32:33,418:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:33,420:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:33,423:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:33,425:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:33,428:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:33,431:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:33,433:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:33,438:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:33,439:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:33,444:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:33,927:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:33,932:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:33,934:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:33,936:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:34,395:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:34,400:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:34,402:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:34,405:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:34,563:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:34,572:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:34,574:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:34,577:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:34,578:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:34,585:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:34,586:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:34,590:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:34,890:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:32:34,893:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:34,895:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:34,896:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:34,897:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:34,898:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:34,899:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:35,035:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:32:35,036:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:35,037:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:35,039:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:35,040:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:35,044:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:35,045:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:35,084:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:32:35,086:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:35,087:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:35,088:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:35,089:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:35,090:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:35,092:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:35,205:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_ranking.py", line 751, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Number of classes in y_true not equal to the number of columns in 'y_score'

  warnings.warn(

2024-09-27 18:32:35,207:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:35,208:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:35,209:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:35,210:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:35,211:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Positive') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-27 18:32:35,212:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-27 18:32:35,221:INFO:Calculating mean and std
2024-09-27 18:32:35,222:INFO:Creating metrics dataframe
2024-09-27 18:32:35,223:INFO:Finalizing model
2024-09-27 18:32:35,625:INFO:Uploading results into container
2024-09-27 18:32:35,625:INFO:Uploading model into container now
2024-09-27 18:32:35,626:INFO:_master_model_container: 18
2024-09-27 18:32:35,626:INFO:_display_container: 6
2024-09-27 18:32:35,626:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-27 18:32:35,626:INFO:create_model() successfully completed......................................
2024-09-27 18:32:35,740:INFO:SubProcess create_model() end ==================================
2024-09-27 18:32:35,741:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False) result for Accuracy is 0.817
2024-09-27 18:32:35,741:INFO:RandomForestClassifier(bootstrap=False, ccp_alpha=0.0, class_weight={},
                       criterion='gini', max_depth=3, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.1, min_samples_leaf=4,
                       min_samples_split=9, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=70, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False) result for Accuracy is 0.817
2024-09-27 18:32:35,742:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False) is best model
2024-09-27 18:32:35,742:INFO:choose_better completed
2024-09-27 18:32:35,743:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2024-09-27 18:32:35,752:INFO:_master_model_container: 18
2024-09-27 18:32:35,753:INFO:_display_container: 5
2024-09-27 18:32:35,753:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-27 18:32:35,753:INFO:tune_model() successfully completed......................................
2024-09-28 13:04:27,587:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.
  warnings.warn(

2024-09-28 13:06:27,034:WARNING:d:\Python\Data Science\Machine Learning\Sentiment Analysis\env\Lib\site-packages\sklearn\metrics\_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.
  warnings.warn(

2024-09-28 13:08:13,336:INFO:Initializing evaluate_model()
2024-09-28 13:08:13,337:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EE7BD150>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-09-28 13:08:14,614:INFO:Initializing plot_model()
2024-09-28 13:08:14,616:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EE7BD150>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-28 13:08:14,617:INFO:Checking exceptions
2024-09-28 13:08:19,578:INFO:Preloading libraries
2024-09-28 13:08:19,692:INFO:Copying training dataset
2024-09-28 13:08:19,694:INFO:Plot type: pipeline
2024-09-28 13:08:23,101:INFO:Visual Rendered Successfully
2024-09-28 13:08:23,820:INFO:plot_model() successfully completed......................................
2024-09-28 13:09:53,518:INFO:Initializing evaluate_model()
2024-09-28 13:09:53,518:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EE7BD150>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-09-28 13:09:56,374:INFO:Initializing plot_model()
2024-09-28 13:09:56,374:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C1EE7BD150>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-28 13:09:56,375:INFO:Checking exceptions
2024-09-28 13:09:59,719:INFO:Preloading libraries
2024-09-28 13:09:59,790:INFO:Copying training dataset
2024-09-28 13:09:59,790:INFO:Plot type: pipeline
2024-09-28 13:10:00,711:INFO:Visual Rendered Successfully
2024-09-28 13:10:01,233:INFO:plot_model() successfully completed......................................
